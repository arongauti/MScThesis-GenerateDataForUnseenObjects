
@incollection{cai_coordinate_2011,
	address = {London},
	title = {Coordinate {Systems} and {Transformations}},
	url = {http://link.springer.com/10.1007/978-0-85729-635-1_2},
	urldate = {2021-04-02},
	booktitle = {Unmanned {Rotorcraft} {Systems}},
	publisher = {Springer London},
	author = {Cai, Guowei and Chen, Ben M. and Lee, Tong Heng},
	collaborator = {Cai, Guowei and Chen, Ben M. and Lee, Tong Heng},
	year = {2011},
	pages = {23--34},
}

@misc{noauthor_melodic_nodate,
	title = {melodic - {ROS} {Wiki}},
	url = {http://wiki.ros.org/melodic},
	urldate = {2021-04-23},
}

@misc{noauthor_python_nodate,
	title = {Python 2.7.0 {Release}},
	url = {https://www.python.org/download/releases/2.7/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2021-04-22},
	journal = {Python.org},
}

@misc{noauthor_mscthesis-generatedataforunseenobjects_nodate,
	title = {{MScThesis}-{GenerateDataForUnseenObjects}},
	url = {https://www.overleaf.com/project/5ffd813835be52ad40d70239},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2021-04-21},
}

@misc{noauthor_topics_nodate,
	title = {Topics - {ROS} {Wiki}},
	url = {http://wiki.ros.org/Topics},
	urldate = {2021-04-21},
}

@misc{noauthor_rostutorialsunderstandingnodes_nodate,
	title = {{ROS}/{Tutorials}/{UnderstandingNodes} - {ROS} {Wiki}},
	url = {http://wiki.ros.org/ROS/Tutorials/UnderstandingNodes},
	urldate = {2021-04-21},
}

@misc{noauthor_master_nodate,
	title = {Master - {ROS} {Wiki}},
	url = {http://wiki.ros.org/Master},
	urldate = {2021-04-21},
}

@misc{noauthor_robot_nodate,
	title = {Robot and interface specifications — {Franka} {Control} {Interface} ({FCI}) documentation},
	url = {https://frankaemika.github.io/docs/control_parameters.html},
	urldate = {2021-04-02},
}

@article{al-naimi_robotics_nodate,
	title = {Robotics and automation},
	language = {en},
	author = {Al-Naimi, Dr Ibrahim},
	pages = {33},
}

@article{zheng_mathematical_1985,
	title = {Mathematical modeling of a robot collision with its environment},
	volume = {2},
	issn = {07412223, 10974563},
	url = {http://doi.wiley.com/10.1002/rob.4620020307},
	doi = {10.1002/rob.4620020307},
	language = {en},
	number = {3},
	urldate = {2021-04-02},
	journal = {Journal of Robotic Systems},
	author = {Zheng, Yuan-Fang and Hemami, Hooshang},
	year = {1985},
	pages = {289--307},
}

@incollection{kaehler_what_2016,
	address = {United States of America},
	edition = {2},
	title = {What {Is} {OpenCV}?},
	url = {https://learning.oreilly.com/library/view/learning-opencv-3/9781491937983/titlepage01.html},
	abstract = {Learning OpenCV 3 Computer Vision in C++ with the OpenCV Library Adrian Kaehler and Gary Bradski},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Learning {OpenCV} 3 - {Computer} {Vision} in {C}++ with the {OpenCV} {Library}},
	publisher = {O'Reilly Media},
	author = {Kaehler, Adrian and Bradski, Gary},
	year = {2016},
	pages = {1},
}

@misc{noauthor_opencv_nodate,
	title = {{OpenCV}},
	url = {https://opencv.org/},
	abstract = {OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning (ML) and Artificial Intelligence (AI).},
	language = {en-US},
	urldate = {2021-04-01},
	journal = {OpenCV},
}

@article{quigley_ros_2009,
	title = {{ROS}: an open-source {Robot} {Operating} {System}},
	volume = {3},
	author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew},
	month = may,
	year = {2009},
	pages = {5},
}

@misc{noauthor_robotworx_nodate,
	title = {{RobotWorx} - {The} {Future} of {Automated} {Random} {Bin} {Picking}},
	url = {https://www.robots.com/blogs/the-future-of-automated-random-bin-picking},
	urldate = {2021-03-26},
}

@misc{noauthor_about_nodate,
	title = {About},
	url = {https://pointcloudlibrary.github.io/about/},
	abstract = {The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Point Cloud Library},
}

@misc{ambalina_5_2020,
	title = {5 {Computer} {Vision} {Companies} to {Follow} in 2020},
	url = {https://lionbridge.ai/articles/5-computer-vision-companies-to-follow-in-2020/},
	abstract = {From AI-powered security cameras to cancer detection and virtual reality, this list will cover 5 computer vision companies building the AI technology of tomorrow.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Lionbridge AI},
	author = {Ambalina, Limarc},
	month = jan,
	year = {2020},
}

@misc{mishra_computer_2021,
	title = {Computer {Vision}: {Image} formation and representation},
	shorttitle = {Computer {Vision}},
	url = {https://towardsdatascience.com/computer-vision-image-formation-and-representation-a63e348e16b4},
	abstract = {Getting started with computer vision},
	language = {en},
	urldate = {2021-03-19},
	journal = {Medium},
	author = {Mishra, Mayank},
	month = jan,
	year = {2021},
}

@incollection{szeliski_introduction_2010,
	title = {Introduction},
	volume = {22},
	isbn = {978-1-84882-935-0},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
	language = {en},
	booktitle = {Computer {Vision}: {Algorithms} and {Applications}},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	month = sep,
	year = {2010},
	note = {Google-Books-ID: bXzAlkODwa8C},
	keywords = {Computers / Computer Graphics, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General},
}

@book{szeliski_computer_2010,
	title = {Computer {Vision}: {Algorithms} and {Applications}},
	isbn = {978-1-84882-935-0},
	shorttitle = {Computer {Vision}},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	month = sep,
	year = {2010},
	note = {Google-Books-ID: bXzAlkODwa8C},
	keywords = {Computers / Computer Graphics, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General},
}

@misc{mihajlovic_everything_nodate,
	title = {Everything {You} {Ever} {Wanted} {To} {Know} {About} {Computer} {Vision}. {\textbar} by {Ilija} {Mihajlovic} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e},
	urldate = {2021-02-24},
	author = {Mihajlovic, Ilija},
}

@incollection{nalwa_what_1993,
	title = {What {Is} {Computer} {Vision}?},
	volume = {3},
	isbn = {978-0-201-54853-2},
	url = {https://books.google.is/books?id=7fJRAAAAMAAJ},
	booktitle = {A {Guided} {Tour} of {Computer} {Vision}},
	publisher = {Addison-Wesley},
	author = {Nalwa, V.S.},
	year = {1993},
	lccn = {92010896},
}

@incollection{nalwa_edge_1993,
	title = {Edge {Detection} and {Image} {Segmentation}},
	volume = {52},
	isbn = {978-0-201-54853-2},
	url = {https://books.google.is/books?id=7fJRAAAAMAAJ},
	booktitle = {A {Guided} {Tour} of {Computer} {Vision}},
	publisher = {Addison-Wesley},
	author = {Nalwa, V.S.},
	year = {1993},
	lccn = {92010896},
}

@article{wilson_relative_1996,
	title = {Relative end-effector control using {Cartesian} position based visual servoing},
	volume = {12},
	issn = {2374-958X},
	doi = {10.1109/70.538974},
	abstract = {This paper presents a complete design methodology for Cartesian position based visual servo control for robots with a single camera mounted at the end-effector. Position based visual servo control requires the explicit calculation of the relative position and orientation (POSE) of the workpiece object with respect to the camera. This is accomplished using image plane measurements of a number of known feature points on the object, and then applying an extended Kalman filter to obtain a recursive solution of the photogrammetric equations, and to properly combine redundant measurements. The control is then designed by specifying the desired trajectories with respect to the object and forming the control error in the end-effector frame. The implementation using a distributed computer architecture is described. An experimental system has been built and used to evaluate the performance of the POSE estimation and the position based visual servo control. Several results for relative trajectory control and target tracking are presented. Results of the experiments showing the effect of loss of some of the redundant features are also presented.},
	number = {5},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Wilson, W. J. and Hulls, C. C. Williams and Bell, G. S.},
	month = oct,
	year = {1996},
	note = {Conference Name: IEEE Transactions on Robotics and Automation},
	keywords = {Cameras, Cartesian position control, Computer architecture, Computer errors, Design methodology, Equations, Error correction, Kalman filter, Kalman filters, POSE estimation, Robot vision systems, Servosystems, Target tracking, Trajectory, feature extraction, manipulator dynamics, optical tracking, photogrammetric equations, position control, relative end-effector control, relative orientation, relative position, robot vision, servomechanisms, target tracking, trajectory control, visual servo control},
	pages = {684--696},
}

@misc{noauthor_inventor_nodate,
	title = {Inventor {\textbar} {Mechanical} {Design} \& {3D} {CAD} {Software} {\textbar} {Autodesk}},
	url = {https://www.autodesk.com/products/inventor/overview, https://www.autodesk.com/products/inventor/overview},
	abstract = {Inventor 3D CAD software is used for product design, rendering, and simulation. Get professional-grade 3D CAD software for product design and engineering.},
	language = {en-US},
	urldate = {2021-03-08},
}

@misc{wallkotter_franka_2020,
	title = {Franka {Emika} {Panda} in {Gazebo} with {ROS} and {Docker}},
	url = {https://sebastianwallkoetter.wordpress.com/2020/03/08/franka-emika-panda-in-gazebo-with-ros-and-docker/},
	abstract = {In my last post I’ve written about creating Gazebo packages from the existing panda models and mentioned that I’m also working on a docker image that includes ROS. Well here it is :) Yo…},
	language = {en},
	urldate = {2021-03-03},
	journal = {Sebastian Wallkötter},
	author = {Wallkötter, Sebastian},
	month = mar,
	year = {2020},
}

@misc{truebenbach_is_2019,
	title = {Is fully automated bin picking finally here?},
	url = {https://www.therobotreport.com/fully-automated-bin-picking-finally-here/},
	abstract = {Universal bin picking needs to be usable by non-experts, quickly configurable, provide path planning with little or no tuning and be cost-effective.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {The Robot Report},
	author = {Truebenbach, Eric},
	month = feb,
	year = {2019},
}

@article{pickett_dont_2018,
	title = {Don't {Fear} the {Cobot}},
	copyright = {Copyright BNP Media Jan 2018},
	issn = {03609936},
	url = {https://search.proquest.com/docview/2052783973/abstract/5B7643FF906E4197PQ/1},
	abstract = {In an interview, robots experts Jeff Burnstein and Kyle Kidwell, talked about some of the advancements, pain points, and opportunities in the current state and not-so-distant future of the robotics industry. Burnstein said SIASUN and other leading Chinese robotics companies have robots for many applications, such as welding, material handling, and assembly. Several, including SIASUN, are also targeting service robots. I recently saw SIASUN products that could be used in elder care, an exciting development given the great need for advances in this area. Kidwell, said also some of the headaches in robot applications are setup and deployment. The typical setup time of a UR robot is around half a day, which is much shorter than most robot applications, as many of them require a lot of work with setting up peripherals and getting them running. He added that Financially speaking, robots have started to emphasize a quick ROI in the manufacturing industry.},
	language = {English},
	urldate = {2021-03-05},
	journal = {Quality},
	author = {Pickett, Leah},
	month = jan,
	year = {2018},
	note = {Num Pages: 12-15
Place: Troy, United States
Publisher: BNP Media
Section: VISION \& SENSORS: VISION ROBOTICS},
	keywords = {Automation, Business And Economics--Management, Engineering, Experts, Manufacturing, Manufacturing engineering, Materials handling, Metrology And Standardization, Robotics, Robots, Service robots, Technological change},
	pages = {12--15},
}

@article{accorsi_application_2019,
	series = {29th {International} {Conference} on {Flexible} {Automation} and {Intelligent} {Manufacturing} ( {FAIM} 2019), {June} 24-28, 2019, {Limerick}, {Ireland}, {Beyond} {Industry} 4.0: {Industrial} {Advances}, {Engineering} {Education} and {Intelligent} {Manufacturing}},
	title = {An application of collaborative robots in a food production facility},
	volume = {38},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978920300457},
	doi = {10.1016/j.promfg.2020.01.044},
	abstract = {Despite the food industry being a leading sector of the European economy, the level of penetration of automation is still low. The main reasons lie on the small margin of food items which does not encourage technological investments, the extremely spread vendors market i.e. mostly small and medium enterprises, and the high level of flexibility and care required to handle food products along production, packaging, and storage operations. Nevertheless, the advent of collaborative, small and flexible robots provides great opportunities for the design and development of new effective processes integrating the human flexibility with the efficiency of automation. This paper explores the impact of adopting collaborative robots in the food catering industry, by illustrating a case study developed for the end-of-line of a catering production system. A generalizable methodology is proposed to support the study of the technical and economic feasibility of the implementation of such technology. This methodology is intended to support managers of the food industry to analyse the constraints that limit the automation of a process and to measure the expected performance of the system in terms of throughput, ergonomics and economic benefits resulting from the adoption of collaborative robots.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Procedia Manufacturing},
	author = {Accorsi, R. and Tufano, A. and Gallo, A. and Galizia, F. G. and Cocchi, G. and Ronzoni, M. and Abbate, A. and Manzini, R.},
	month = jan,
	year = {2019},
	keywords = {Collaborative robots, Feasibility analysis, Food industry, Industry 4.0, Production automatization},
	pages = {341--348},
}

@article{schmidbauer_teaching_2020,
	series = {Learning {Factories} across the value chain – from innovation to service – {The} 10th {Conference} on {Learning} {Factories} 2020},
	title = {Teaching {Cobots} in {Learning} {Factories} – {User} and {Usability}-{Driven} {Implications}},
	volume = {45},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978920310817},
	doi = {10.1016/j.promfg.2020.04.043},
	abstract = {Up to now, industrial robots were considered as machines working for humans. In this sense, programming required special coding knowledge and skills as teaching approaches were based on imperative programming methods, formulating the solving of a problem line by line. As novel work principles consider the robot rather a tool, assisting or collaborating with the human, declarative approaches are needed, that allow for intuitiveness and modifiability. Collaborative robot (cobot) control requires intuitive interfaces, not only for ease of use but also for modifying existing execution programs. Furthermore, the increasingly diverse personnel also requires a more democratic approach to robot programming. However, there is no standard or guideline for intuitive cobot control, and it can be noticed that the usability of the diverse interfaces and systems provided on the market is rather poor. This paper compares three systems with their advantages and disadvantages concerning usability and presents the results of the standard usability score (SUS) test, which was conducted in the Vienna learning factory “TU Wien Pilot Factory for Industry 4.0”. Additionally, the paper presents an approach on how to teach different levels of cobot interaction and control addressing the skill sets needed in their individual working environments.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Procedia Manufacturing},
	author = {Schmidbauer, Christina and Komenda, Titanilla and Schlund, Sebastian},
	month = jan,
	year = {2020},
	keywords = {Cobot Teaching, Collaborative Robots, Learning Concept, Robot Programming, Usability},
	pages = {398--404},
}

@article{el_zaatari_cobot_2019,
	title = {Cobot programming for collaborative industrial tasks: {An} overview},
	volume = {116},
	issn = {0921-8890},
	shorttitle = {Cobot programming for collaborative industrial tasks},
	url = {https://www.sciencedirect.com/science/article/pii/S092188901830602X},
	doi = {10.1016/j.robot.2019.03.003},
	abstract = {Collaborative robots (cobots) have been increasingly adopted in industries to facilitate human–robot collaboration. Despite this, it is challenging to program cobots for collaborative industrial tasks as the programming has two distinct elements that are difficult to implement: (1) an intuitive element to ensure that the operations of a cobot can be composed or altered dynamically by an operator, and (2) a human-aware element to support cobots in producing flexible and adaptive behaviours dependent on human partners. In this area, some research works have been carried out recently, but there is a lack of a systematic summary on the subject. In this paper, an overview of collaborative industrial scenarios and programming requirements for cobots to implement effective collaboration is given. Then, detailed reviews on cobot programming, which are categorised into communication, optimisation, and learning, are conducted. Additionally, a significant gap between cobot programming implemented in industry and in research is identified, and research that works towards bridging this gap is pinpointed. Finally, the future directions of cobots for industrial collaborative scenarios are outlined, including potential points of extension and improvement.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Robotics and Autonomous Systems},
	author = {El Zaatari, Shirine and Marei, Mohamed and Li, Weidong and Usman, Zahid},
	month = jun,
	year = {2019},
	keywords = {Cobot, Human-awareness, Human–robot collaboration, Intuitive programming},
	pages = {162--180},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	urldate = {2021-03-04},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
}

@misc{noauthor_machine_nodate,
	title = {Machine loading},
	url = {https://wiredworkers.io/applications/machine-loading/},
	abstract = {Cobots can be used to take over tasks from employees. One of these dangerous and time-consuming tasks is loading production machines.},
	language = {en-US},
	urldate = {2021-03-02},
	journal = {WiredWorkers},
}

@article{gaz_dynamic_2019,
	title = {Dynamic {Identification} of the {Franka} {Emika} {Panda} {Robot} {With} {Retrieval} of {Feasible} {Parameters} {Using} {Penalty}-{Based} {Optimization}},
	doi = {10.1109/LRA.2019.2931248},
	abstract = {In this letter, we address the problem of extracting a feasible set of dynamic parameters characterizing the dynamics of a robot manipulator. We start by identifying through an ordinary least squares approach the dynamic coefficients that linearly parametrize the model. From these, we retrieve a set of feasible link parameters (mass, position of center of mass, inertia) that is fundamental for more realistic dynamic simulations or when implementing in real time robot control laws using recursive Newton-Euler algorithms. The resulting problem is solved by means of an optimization method that incorporates constraints on the physical consistency of the dynamic parameters, including the triangle inequality of the link inertia tensors as well as other user-defined, possibly nonlinear constraints. The approach is developed for the increasingly popular Panda robot by Franka Emika, identifying for the first time its dynamic coefficients, an accurate joint friction model, and a set of feasible dynamic parameters. Validation of the identified dynamic model and of the retrieved feasible parameters is presented for the inverse dynamics problem using, respectively, a Lagrangian approach and Newton-Euler computations.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gaz, Claudio and Cognetti, Marco and Oliva, A. and Giordano, Paolo Robuffo and Luca, A. De},
	year = {2019},
}

@misc{noauthor_opencv_nodate-1,
	title = {{OpenCV}: {Contours} : {Getting} {Started}},
	url = {https://docs.opencv.org/master/d4/d73/tutorial_py_contours_begin.html},
	urldate = {2021-02-25},
}

@misc{noauthor_yolo_nodate,
	title = {{YOLO}: {Real}-{Time} {Object} {Detection}},
	url = {https://pjreddie.com/darknet/yolo/},
	urldate = {2021-02-25},
}

@misc{gmbh_franka_nodate,
	title = {Franka {Emika}},
	url = {https://www.franka.de/technology},
	abstract = {The robot for everyone – sensitive, interconnected, adaptive and cost-efficient.},
	language = {en-US},
	urldate = {2021-02-24},
	author = {GmbH, Franka Emika},
}

@misc{noauthor_c_nodate,
	title = {C (programming language) - encyclopedia article - {Citizendium}},
	url = {https://en.citizendium.org/wiki/C_(programming_language)},
	urldate = {2021-02-24},
}

@misc{noauthor_what_nodate,
	title = {What is {Python}? {Executive} {Summary}},
	shorttitle = {What is {Python}?},
	url = {https://www.python.org/doc/essays/blurb/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2021-02-24},
	journal = {Python.org},
}

@misc{noauthor_ubuntu_nodate,
	title = {Ubuntu release cycle},
	url = {https://ubuntu.com/about/release-cycle},
	abstract = {Overview of the Ubuntu release cycle - maintenance, support and security coverage, lifetime, upgrade paths, kernel versions and the range of editions and images published by Canonical.},
	language = {en},
	urldate = {2021-02-24},
	journal = {Ubuntu},
}

@misc{noauthor_lightnet_nodate,
	title = {Lightnet documentation — {Lightnet} 2.0.1 documentation},
	url = {https://eavise.gitlab.io/lightnet/},
	urldate = {2021-02-24},
}

@misc{noauthor_ultralyticsyolov3_2021,
	title = {ultralytics/yolov3},
	copyright = {GPL-3.0 License         ,                 GPL-3.0 License},
	url = {https://github.com/ultralytics/yolov3},
	abstract = {YOLOv3 in PyTorch {\textgreater} ONNX {\textgreater} CoreML {\textgreater} TFLite. Contribute to ultralytics/yolov3 development by creating an account on GitHub.},
	urldate = {2021-02-24},
	publisher = {Ultralytics LLC},
	month = feb,
	year = {2021},
	note = {original-date: 2018-08-26T08:57:20Z},
	keywords = {coreml, deep-learning, ios, machine-learning, ml, object-detection, onnx, pytorch, tflite, yolo, yolov3, yolov4, yolov5},
}

@misc{noauthor_darknet_nodate,
	title = {Darknet: {Open} {Source} {Neural} {Networks} in {C}},
	url = {https://pjreddie.com/darknet/},
	urldate = {2021-02-24},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding darknet's yolo.cfg config files},
	url = {https://stackoverflow.com/questions/50390836/understanding-darknets-yolo-cfg-config-files},
	urldate = {2021-02-24},
	journal = {Stack Overflow},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2021-02-24},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brooks_jsbrokscoco-annotator_2021,
	title = {jsbroks/coco-annotator},
	copyright = {MIT License         ,                 MIT License},
	url = {https://github.com/jsbroks/coco-annotator},
	abstract = {:pencil2: Web-based image segmentation tool for object detection, localization, and keypoints},
	urldate = {2021-02-24},
	author = {Brooks, Justin},
	month = feb,
	year = {2021},
	note = {original-date: 2018-09-03T21:38:31Z},
	keywords = {annotate-images, coco, coco-annotator, coco-format, computer-vision, datasets, deep-learning, detection, image-annotation, image-labeling, image-segmentation, label, machine-learning},
}

@inproceedings{rahimi_industrial_2017,
	title = {An {Industrial} {Robotics} {Application} with {Cloud} {Computing} and {High}-{Speed} {Networking}},
	doi = {10.1109/IRC.2017.39},
	abstract = {This paper describes an industrial cloud robotics distributed application that was executed across a high-speed wide-area network. The application was implemented using ROS libraries and packages. The purpose of the application is to enable an industrial robot to perform surface blending. A Kinect sensor, a surface blending tool and a laser scanner are mounted on the robot arm. The arm is moved under software control to scan a work bench on which metal parts of variable size can be laid out at any orientation. The collected point cloud data is processed by a segmentation algorithm to find the surface boundaries. A Cartesian path planning algorithm is executed to determine paths for the robot arm to execute the blending action and a laser scan on a selected surface. A new ROS package was implemented to collect CPU, memory and bandwidth usage for each significant ROS node in this distributed application. To emulate a scenario in which computing resources at a remote datacenter can be used for the segmentation and path planning algorithms in conjunction with the robots located on a factory floor, a software-defined network testbed called GENI was used to distribute compute-heavy ROS nodes. Measurements show that with TCP tuning, and high-speed end-to-end paths, the total execution time in the Cloud scenario can be reasonably close to a local scenario in which computing is collocated with the robot.},
	booktitle = {2017 {First} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Rahimi, R. and Shao, C. and Veeraraghavan, M. and Fumagalli, A. and Nicho, J. and Meyer, J. and Edwards, S. and Flannigan, C. and Evans, P.},
	month = apr,
	year = {2017},
	keywords = {Cartesian path planning algorithm, Cloud computing, GENI, Global Environment for Network Innovations, High-speed networks, Industrial robotics, Kinect sensor, Manipulators, Metals, ROS, ROS libraries, ROS packages, ROS-I, Robot sensing systems, Service robots, Surface treatment, TCP tuning, cloud computing, control engineering computing, high-speed networking, image segmentation, industrial robotics application, industrial robots, laser scanner, manipulators, operating systems (computers), path planning, production engineering computing, robot arm, segmentation algorithm, software defined networking, software-defined network testbed, surface blending tool, surface boundaries, wide area networks, wide-area network},
	pages = {44--51},
}

@article{chitta_moveitros_2012,
	title = {Moveit![{ROS} topics]},
	volume = {19},
	doi = {10.1109/MRA.2011.2181749},
	journal = {IEEE Robotics \& Automation Magazine - IEEE ROBOT AUTOMAT},
	author = {Chitta, Sachin and Sucan, Ioan and Cousins, Steve},
	month = mar,
	year = {2012},
	pages = {18--19},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-02-24},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
}

@misc{noauthor_ros_nodate,
	title = {{ROS} {Index}},
	url = {https://index.ros.org/packages/},
	urldate = {2021-02-24},
}

@misc{noauthor_rosorg_nodate,
	title = {{ROS}.org {\textbar} {Is} {ROS} {For} {Me}?},
	url = {https://www.ros.org/is-ros-for-me/},
	language = {en-US},
	urldate = {2021-02-24},
}

@misc{noauthor_robotics_nodate,
	title = {Robotics {Industry} {Insights} - {Robotic} {Bin} {Picking} – {The}...},
	url = {https://www.robotics.org/content-detail.cfm/Industrial-Robotics-Industry-Insights/Robotic-Bin-Picking-The-Holy-Grail-in-Sight/content_id/6002},
	urldate = {2021-02-24},
}

@book{buchholz_bin-picking_2015,
	title = {Bin-{Picking}: {New} {Approaches} for a {Classical} {Problem}},
	isbn = {978-3-319-26500-1},
	shorttitle = {Bin-{Picking}},
	abstract = {This book is devoted to one of the most famous examples of automation handling tasks – the “bin-picking” problem. To pick up objects, scrambled in a box is an easy task for humans, but its automation is very complex. In this book three different approaches to solve the bin-picking problem are described, showing how modern sensors can be used for efficient bin-picking as well as how classic sensor concepts can be applied for novel bin-picking techniques. 3D point clouds are firstly used as basis, employing the known Random Sample Matching algorithm paired with a very efficient depth map based collision avoidance mechanism resulting in a very robust bin-picking approach. Reducing the complexity of the sensor data, all computations are then done on depth maps. This allows the use of 2D image analysis techniques to fulfill the tasks and results in real time data analysis. Combined with force/torque and acceleration sensors, a near time optimal bin-picking system emerges. Lastly, surface normal maps are employed as a basis for pose estimation. In contrast to known approaches, the normal maps are not used for 3D data computation but directly for the object localization problem, enabling the application of a new class of sensors for bin-picking.},
	language = {en},
	publisher = {Springer},
	author = {Buchholz, Dirk},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: BfYUCwAAQBAJ},
	keywords = {Computers / Computer Graphics, Computers / Intelligence (AI) \& Semantics, Computers / Optical Data Processing, Technology \& Engineering / Engineering (General), Technology \& Engineering / General, Technology \& Engineering / Robotics},
}

@misc{noauthor_working_2017,
	title = {Working with {Images} in {Python}},
	url = {https://www.geeksforgeeks.org/working-images-python/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {GeeksforGeeks},
	month = jan,
	year = {2017},
	note = {Section: Python},
}

@misc{noauthor_intel_nodate,
	title = {Intel® {RealSense}™ {LiDAR} {Camera} {L515}},
	url = {https://www.intelrealsense.com/lidar-camera-l515/},
	abstract = {World's smallest high-resolution LiDAR camera from Intel®. Low power and low weight, open-source SDK.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {Intel® RealSense™ Depth and Tracking Cameras},
}
