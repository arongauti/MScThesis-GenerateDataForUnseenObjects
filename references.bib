
@misc{datta_all_2021,
	title = {All about {Structural} {Similarity} {Index} ({SSIM}): {Theory} + {Code} in {PyTorch}},
	shorttitle = {All about {Structural} {Similarity} {Index} ({SSIM})},
	url = {https://medium.com/srm-mic/all-about-structural-similarity-index-ssim-theory-code-in-pytorch-6551b455541e},
	abstract = {Structural Similarity Index, theory and code explained in depth with the help of a PyTorch implementation.},
	language = {en},
	urldate = {2021-05-23},
	journal = {Medium},
	author = {Datta, Pranjal},
	month = mar,
	year = {2021},
}

@incollection{cai_coordinate_2011,
	address = {London},
	title = {Coordinate {Systems} and {Transformations}},
	urldate = {2021-04-02},
	booktitle = {Unmanned {Rotorcraft} {Systems}},
	publisher = {Springer London},
	author = {Cai, Guowei and Chen, Ben M. and Lee, Tong Heng},
	collaborator = {Cai, Guowei and Chen, Ben M. and Lee, Tong Heng},
	year = {2011},
	pages = {23--34},
}

@incollection{kaehler_what_2016,
	address = {United States of America},
	edition = {2},
	title = {What {Is} {OpenCV}?},
	abstract = {Learning OpenCV 3 Computer Vision in C++ with the OpenCV Library Adrian Kaehler and Gary Bradski},
	language = {en},
	urldate = {2021-04-01},
	booktitle = {Learning {OpenCV} 3 - {Computer} {Vision} in {C}++ with the {OpenCV} {Library}},
	publisher = {O'Reilly Media},
	author = {Kaehler, Adrian and Bradski, Gary},
	year = {2016},
	pages = {1},
}

@book{szeliski_computer_2010,
	title = {Computer {Vision}: {Algorithms} and {Applications}},
	isbn = {978-1-84882-935-0},
	shorttitle = {Computer {Vision}},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	month = sep,
	year = {2010},
	keywords = {Computers / Computer Graphics, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General},
}

@article{liu_detection_2020,
	title = {A detection and recognition system of pointer meters in substations based on computer vision},
	volume = {152},
	issn = {0263-2241},
	url = {https://www.sciencedirect.com/science/article/pii/S0263224119311972},
	doi = {10.1016/j.measurement.2019.107333},
	abstract = {In order to develop unattended intelligent substations, numerous automatic meter reading methods have been proposed with the invention of inspection robots. However, most methods have strict restrictions on the captured image quality, so they cannot meet the application requirements of the substation. In this paper, a versatile pointer meter detection and recognition system based on computer vision is proposed for different environments. A Faster Region-based Convolutional Network (Faster R-CNN) is first used to detect the position of the target meter and the camera is adjusted according to the detection box. Then, thanks to the feature correspondence algorithm and the Perspective Transform, the problem of specular reflection and image distortion is solved to obtain high quality images. Finally, reading can be obtained after the position of pointer detected by Hough Transform. The experimental results verify the stability and accuracy of recognition system which is proved to work well under different conditions.},
	language = {en},
	urldate = {2021-05-20},
	journal = {Measurement},
	author = {Liu, Yang and Liu, Jun and Ke, Yichen},
	month = feb,
	year = {2020},
	keywords = {Computer vision, Detection and recognition, Feature correspondence, Perspective transform, Pointer meter},
	pages = {107333},
}

@article{liu_detection_2020-1,
	title = {A detection and recognition system of pointer meters in substations based on computer vision},
	volume = {152},
	issn = {0263-2241},
	url = {https://www.sciencedirect.com/science/article/pii/S0263224119311972},
	doi = {10.1016/j.measurement.2019.107333},
	abstract = {In order to develop unattended intelligent substations, numerous automatic meter reading methods have been proposed with the invention of inspection robots. However, most methods have strict restrictions on the captured image quality, so they cannot meet the application requirements of the substation. In this paper, a versatile pointer meter detection and recognition system based on computer vision is proposed for different environments. A Faster Region-based Convolutional Network (Faster R-CNN) is first used to detect the position of the target meter and the camera is adjusted according to the detection box. Then, thanks to the feature correspondence algorithm and the Perspective Transform, the problem of specular reflection and image distortion is solved to obtain high quality images. Finally, reading can be obtained after the position of pointer detected by Hough Transform. The experimental results verify the stability and accuracy of recognition system which is proved to work well under different conditions.},
	language = {en},
	urldate = {2021-05-20},
	journal = {Measurement},
	author = {Liu, Yang and Liu, Jun and Ke, Yichen},
	month = feb,
	year = {2020},
	keywords = {Computer vision, Detection and recognition, Feature correspondence, Perspective transform, Pointer meter},
	pages = {107333},
}

@article{manogaran_wearable_2019,
	title = {Wearable {IoT} {Smart}-{Log} {Patch}: {An} {Edge} {Computing}-{Based} {Bayesian} {Deep} {Learning} {Network} {System} for {Multi} {Access} {Physical} {Monitoring} {System}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Wearable {IoT} {Smart}-{Log} {Patch}},
	url = {https://www.mdpi.com/1424-8220/19/13/3030},
	doi = {10.3390/s19133030},
	abstract = {According to the survey on various health centres, smart log-based multi access physical monitoring system determines the health conditions of humans and their associated problems present in their lifestyle. At present, deficiency in significant nutrients leads to deterioration of organs, which creates various health problems, particularly for infants, children, and adults. Due to the importance of a multi access physical monitoring system, children and adolescents’ physical activities should be continuously monitored for eliminating difficulties in their life using a smart environment system. Nowadays, in real-time necessity on multi access physical monitoring systems, information requirements and the effective diagnosis of health condition is the challenging task in practice. In this research, wearable smart-log patch with Internet of Things (IoT) sensors has been designed and developed with multimedia technology. Further, the data computation in that smart-log patch has been analysed using edge computing on Bayesian deep learning network (EC-BDLN), which helps to infer and identify various physical data collected from the humans in an accurate manner to monitor their physical activities. Then, the efficiency of this wearable IoT system with multimedia technology is evaluated using experimental results and discussed in terms of accuracy, efficiency, mean residual error, delay, and less energy consumption. This state-of-the-art smart-log patch is considered as one of evolutionary research in health checking of multi access physical monitoring systems with multimedia technology.},
	language = {en},
	number = {13},
	urldate = {2021-05-20},
	journal = {Sensors},
	author = {Manogaran, Gunasekaran and Shakeel, P. Mohamed and Fouad, H. and Nam, Yunyoung and Baskar, S. and Chilamkurti, Naveen and Sundarasekar, Revathi},
	month = jan,
	year = {2019},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian neural network, edge computing, multi access physical monitoring system, multimedia technology, smart-log patch},
	pages = {3030},
}

@article{alfarraj_two-level_2020,
	title = {A two-level computer vision-based information processing method for improving the performance of human–machine interaction-aided applications},
	issn = {2198-6053},
	url = {https://doi.org/10.1007/s40747-020-00208-6},
	doi = {10.1007/s40747-020-00208-6},
	abstract = {The computer vision (CV) paradigm is introduced to improve the computational and processing system efficiencies through visual inputs. These visual inputs are processed using sophisticated techniques for improving the reliability of human–machine interactions (HMIs). The processing of visual inputs requires multi-level data computations for achieving application-specific reliability. Therefore, in this paper, a two-level visual information processing (2LVIP) method is introduced to meet the reliability requirements of HMI applications. The 2LVIP method is used for handling both structured and unstructured data through classification learning to extract the maximum gain from the inputs. The introduced method identifies the gain-related features on its first level and optimizes the features to improve information gain. In the second level, the error is reduced through a regression process to stabilize the precision to meet the HMI application demands. The two levels are interoperable and fully connected to achieve better gain and precision through the reduction in information processing errors. The analysis results show that the proposed method achieves 9.42\% higher information gain and a 6.51\% smaller error under different classification instances compared with conventional methods.},
	language = {en},
	urldate = {2021-05-20},
	journal = {Complex \& Intelligent Systems},
	author = {Alfarraj, Osama and Tolba, Amr},
	month = oct,
	year = {2020},
}

@article{guo_lossy_2018,
	title = {Lossy {Compression} for {Embedded} {Computer} {Vision} {Systems}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2852809},
	abstract = {Computer vision applications are rapidly gaining popularity in embedded systems, which typically involve a difficult tradeoff between vision performance and energy consumption under a constraint of real-time processing throughput. Recently, hardware (FPGA and ASIC-based) implementations have emerged, which significantly improves the energy efficiency of vision computation. These implementations, however, often involve intensive memory traffic that retains a significant portion of energy consumption at the system level. To address this issue, we are the first researchers to present a lossy compression framework to exploit the tradeoff between vision performance and memory traffic for input images. To meet various requirements for memory access patterns in the vision system, a line-to-block format conversion is designed for the framework. Differential pulse-code modulation-based gradient-oriented quantization is developed as the lossy compression algorithm. We also present its hardware design that supports up to 12-scale 1080p@60fps real-time processing. For histogram of oriented gradient-based deformable part models on VOC2007, the proposed framework achieves a 49.6\%-60.5\% memory traffic reduction at a detection rate degradation of 0.05\%-0.34\%. For AlexNet on ImageNet, memory traffic reduction achieves up to 60.8\% with less than 0.61\% classification rate degradation. Compared with the power consumption reduction from memory traffic, the overhead involved for the proposed input image compression is less than 5\%.},
	journal = {IEEE Access},
	author = {Guo, Li and Zhou, Dajiang and Zhou, Jinjia and Kimura, Shinji and Goto, Satoshi},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Computer vision, Feature extraction, Hardware, Image coding, Memory management, Power demand, Random access memory, feature extraction, lossy compression, memory traffic reduction},
	pages = {39385--39397},
}

@article{zeng_robotic_2019,
	title = {Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364919868017},
	doi = {10.1177/0278364919868017},
	abstract = {This article presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses an object-agnostic grasping framework to map from visual observations to actions: inferring dense pixel-wise probability maps of the affordances for four different grasping primitive actions. It then executes the action with the highest affordance and recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional data collection or re-training. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT–Princeton Team system that took first place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http://arc.cs.princeton.edu/},
	language = {en},
	urldate = {2021-05-20},
	journal = {The International Journal of Robotics Research},
	author = {Zeng, Andy and Song, Shuran and Yu, Kuan-Ting and Donlon, Elliott and Hogan, Francois R. and Bauza, Maria and Ma, Daolin and Taylor, Orion and Liu, Melody and Romo, Eudald and Fazeli, Nima and Alet, Ferran and Chavan Dafle, Nikhil and Holladay, Rachel and Morona, Isabella and Nair, Prem Qu and Green, Druck and Taylor, Ian and Liu, Weber and Funkhouser, Thomas and Rodriguez, Alberto},
	month = aug,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Amazon Robotics Challenge, active perception, affordance learning, cross-domain image matching, deep learning, grasping, one-shot recognition, pick-and-place, vision for manipulation},
	pages = {0278364919868017},
}

@article{xie_unseen_2021,
	title = {Unseen {Object} {Instance} {Segmentation} for {Robotic} {Environments}},
	issn = {1941-0468},
	doi = {10.1109/TRO.2021.3060341},
	abstract = {In order to function in unstructured environments, robots need the ability to recognize unseen objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. Our proposed method, unseen object instance segmentation (UOIS)-Net, separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is composed of two stages: first, it operates only on depth to produce object instance center votes in 2D or 3D and assembles them into rough initial masks. Second, these initial masks are refined using RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is nonphotorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method can produce sharp and accurate segmentation masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping.},
	journal = {IEEE Transactions on Robotics},
	author = {Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Image segmentation, Noise measurement, Robot perception, Robots, Semantics, Three-dimensional displays, Training, Two dimensional displays, sim-to-real, unseen object instance segmentation},
	pages = {1--17},
}

@article{girshick_region-based_2016,
	title = {Region-{Based} {Convolutional} {Networks} for {Accurate} {Object} {Detection} and {Segmentation}},
	volume = {38},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2437384},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Detectors, Feature extraction, Image segmentation, Object detection, Object recognition, Proposals, Support vector machines, Training, convolutional networks, deep learning, detection, semantic segmentation, transfer learning},
	pages = {142--158},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2021-05-13},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	note = {ISSN: 1063-6919},
	keywords = {High performance computing, Histograms, Humans, Image databases, Image edge detection, Object detection, Object recognition, Robustness, Support vector machines, Testing},
	pages = {886--893 vol. 1},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-05-13},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-05-13},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{elfouly_r-cnn_2020,
	title = {R-{CNN}},
	url = {https://medium.com/@selfouly/r-cnn-3a9beddfd55a},
	abstract = {A beginners guide to one of the most fundamental concepts in object detection.},
	language = {en},
	urldate = {2021-05-13},
	journal = {Medium},
	author = {Elfouly, Sharif},
	month = nov,
	year = {2020},
}

@misc{noauthor_what_nodate,
	title = {What are {ConvNets}? - {Quora}},
	url = {https://www.quora.com/What-are-ConvNets},
	urldate = {2021-05-05},
}

@misc{bansari_introduction_2019,
	title = {Introduction to how {CNNs} {Work}},
	url = {https://medium.datadriveninvestor.com/introduction-to-how-cnns-work-77e0e4cde99b},
	abstract = {Introduction to how CNNs work},
	language = {en},
	urldate = {2021-05-05},
	journal = {Medium},
	author = {Bansari, Simran},
	month = apr,
	year = {2019},
}

@article{liu_implementation_2015,
	title = {Implementation of {Training} {Convolutional} {Neural} {Networks}},
	abstract = {Deep learning refers to the shining branch of machine learning that is based on learning levels of representations. Convolutional Neural Networks (CNN) is one kind of deep neural network. It can study concurrently. In this article, we gave a detailed analysis of the process of CNN algorithm both the forward process and back propagation. Then we applied the particular convolutional neural network to implement the typical face recognition problem by java. Then, a parallel strategy was proposed in section4. In addition, by measuring the actual time of forward and backward computing, we analysed the maximal speed up and parallel efficiency theoretically.},
	language = {en},
	author = {Liu, Tianyi and Fang, Shuangsang and Zhao, Yuehui and Wang, Peng and Zhang, Jun},
	year = {2015},
	pages = {10},
}

@misc{bhatia_why_2018,
	title = {Why {Convolutional} {Neural} {Networks} {Are} {The} {Go}-{To} {Models} {In} {DL}},
	url = {https://analyticsindiamag.com/why-convolutional-neural-networks-are-the-go-to-models-in-deep-learning/},
	abstract = {Over the years, research on convolutional neural networks (CNNs) has progressed rapidly, however the real-world deployment of these models is often limited by computing resources and memory constraints.},
	language = {en-US},
	urldate = {2021-05-05},
	journal = {Analytics India Magazine},
	author = {Bhatia, Richa},
	month = sep,
	year = {2018},
}

@misc{e_convolutional_2020,
	title = {Convolutional {Neural} {Network} {\textbar} {Deep} {Learning}},
	url = {https://developersbreach.com/convolution-neural-network-deep-learning/},
	abstract = {CNN are very powerful and widely used in image classification, image recognition, computer vision etc and is deep learning neural network.},
	language = {en-US},
	urldate = {2021-05-05},
	journal = {Developers Breach},
	author = {E, Swapna K.},
	month = aug,
	year = {2020},
}

@incollection{nalwa_what_1993,
	title = {What {Is} {Computer} {Vision}?},
	volume = {3},
	isbn = {978-0-201-54853-2},
	booktitle = {A {Guided} {Tour} of {Computer} {Vision}},
	publisher = {Addison-Wesley},
	author = {Nalwa, V.S.},
	year = {1993},
	lccn = {92010896},
}

@book{zou_object_2019,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
}

@incollection{nalwa_edge_1993,
	title = {Edge {Detection} and {Image} {Segmentation}},
	volume = {52},
	isbn = {978-0-201-54853-2},
	booktitle = {A {Guided} {Tour} of {Computer} {Vision}},
	publisher = {Addison-Wesley},
	author = {Nalwa, V.S.},
	year = {1993},
	lccn = {92010896},
}

@misc{noauthor_open_nodate,
	title = {Open {Images} {Dataset}},
	url = {https://opensource.google/projects/},
	abstract = {Learn about all our projects.},
	language = {en},
	urldate = {2021-05-02},
	journal = {opensource.google},
}

@misc{noauthor_coco_nodate,
	title = {{COCO} - {Common} {Objects} in {Context}},
	url = {https://cocodataset.org/#home},
	urldate = {2021-05-02},
}

@phdthesis{bjarnason_1984-_detecting_2021,
	type = {Thesis},
	title = {Detecting previously unseen objects without human intervention using neural networks.},
	url = {https://skemman.is/handle/1946/37528},
	abstract = {Automation entails many previously unforeseen problems. As the unfathomable processing power of the human mind creates the illusion that detecting and picking up objects is,in some way, an easy task. Distinguishing objects from other identical objects, often only in one color, is not as easy as one might think.The task at hand is to reduce the time it takes to train a neural network to recognize each object in large warehouses, and annotate thousands of images to do so.Going through the process of finding a good dataset to use in a series of experiments, this thesis aims to answer the question if it is possible to use a minimal dataset to train a neural network that can detect a more comprehensive array of objects. Through incremental and batch learning procedure, experiments are done with different methods of training and detecting objects. From the results of these experiments, it can be concluded that there is a possibility that a network can be made to detect a large variety of objects by only training the network on a portion of the catalog at hand.},
	language = {en},
	urldate = {2021-05-01},
	author = {Bjarnason 1984-, Sverrir},
	month = jan,
	year = {2021},
	note = {Accepted: 2021-01-25T14:24:59Z},
}

@article{buckland_relationship_1994,
	title = {The relationship between {Recall} and {Precision}},
	volume = {45},
	copyright = {Copyright © 1994 John Wiley \& Sons, Inc.},
	issn = {1097-4571},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199401%2945%3A1%3C12%3A%3AAID-ASI2%3E3.0.CO%3B2-L},
	doi = {https://doi.org/10.1002/(SICI)1097-4571(199401)45:1<12::AID-ASI2>3.0.CO;2-L},
	abstract = {Empirical studies of retrieval performance have shown a tendency for Precision to decline as Recall increases. This article examines the nature of the relationship between Precision and Recall. The relationships between Recall and the number of documents retrieved, between Precision and the number of documents retrieved, and between Precision and Recall are described in the context of different assumptions about retrieval performance. It is demonstrated that a tradeoff between Recall and Precision is unavoidable whenever retrieval performance is consistently better than retrieval at random. More generally, for the Precision–Recall trade-off to be avoided as the total number of documents retrieved increases, retrieval performance must be equal to or better than overall retrieval performance up to that point. Examination of the mathematical relationship between Precision and Recall shows that a quadratic Recall curve can resemble empirical Recall–Precision behavior if transformed into a tangent parabola. With very large databases and/or systems with limited retrieval capabilities there can be advantages to retrieval in two stages: Initial retrieval emphasizing high Recall, followed by more detailed searching of the initially retrieved set, can be used to improve both Recall and Precision simultaneously. Even so, a tradeoff between Precision and Recall remains. © 1994 John Wiley \& Sons, Inc.},
	language = {en},
	number = {1},
	urldate = {2021-04-29},
	journal = {Journal of the American Society for Information Science},
	author = {Buckland, Michael and Gey, Fredric},
	year = {1994},
	pages = {12--19},
}

@misc{noauthor_thinkcentre_nodate,
	title = {{ThinkCentre} {M83} {SFF} {Pro} {Desktop} {\textbar} {Power} \& {Stability} {\textbar} {Lenovo} {India}},
	url = {https://www.lenovo.com/in/en/desktops/thinkcentre/m-series-sff/m83/},
	abstract = {Discover the ThinkCentre M83 small form factor pro desktop, powered for productivity with new 4th gen Intel\&reg; Core\&trade; processors, advanced wireless, easy expansion, 27-month life cycle, and more.},
	language = {en},
	urldate = {2021-04-30},
}

@misc{noauthor_deep_2018,
	title = {Deep {Learning} {GPU} {Benchmarks} - {Tesla} {V100} vs {RTX} 2080 {Ti} vs {GTX} 1080 {Ti} vs {Titan} {V}},
	url = {https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/},
	abstract = {What's the best GPU for Deep Learning in 2019? We benchmark the 2080 Ti vs the Titan V, V100, and 1080 Ti.},
	language = {en},
	urldate = {2021-04-30},
	journal = {Lambda Blog},
	month = oct,
	year = {2018},
}

@misc{noauthor_graphics_nodate,
	title = {Graphics {Reinvented}: {NVIDIA} {GeForce} {RTX} 2080 {Ti} {Graphics} {Card}},
	shorttitle = {Graphics {Reinvented}},
	url = {https://www.nvidia.com/en-eu/geforce/graphics-cards/rtx-2080-ti/},
	abstract = {A revolution in gaming realism and performance. Latest NVIDIA Turing GPU architecture, 11 GB of next-gen, ultra-fast GDDR6 memory. The world’s ultimate gaming graphics card.},
	language = {en-eu},
	urldate = {2021-04-30},
	journal = {NVIDIA},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2021-04-30},
	journal = {arXiv:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{choudhury_top_2020,
	title = {Top 8 {Algorithms} {For} {Object} {Detection} {One} {Must} {Know}},
	url = {https://analyticsindiamag.com/top-8-algorithms-for-object-detection/},
	abstract = {Object detection has been witnessing a rapid revolutionary change in the field of computer vision and considered as a complex topic.},
	language = {en-US},
	urldate = {2021-04-30},
	journal = {Analytics India Magazine},
	author = {Choudhury, Ambika},
	month = jun,
	year = {2020},
}

@article{zou_object_2019-1,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	urldate = {2021-04-30},
	journal = {arXiv:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_create_nodate,
	title = {Create {COCO} {Annotations} {From} {Scratch}},
	url = {https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch},
	abstract = {This tutorial will teach you how to create a simple COCO-like dataset from scratch. It gives example code and example JSON annotations.},
	language = {en-US},
	urldate = {2021-04-29},
	journal = {Immersive Limit},
}

@misc{noauthor_f1_nodate,
	title = {F1 {Score} {Precision} {Recall}},
	url = {https://formulaf1results.blogspot.com/2020/10/f1-score-precision-recall.html},
	urldate = {2021-04-29},
}

@misc{mittapally_whats_2019,
	title = {What’s {Recall} and {Precision}?},
	url = {https://becominghuman.ai/whats-recall-and-precision-4a801b1ac0da},
	abstract = {These are the most widely used model evaluation metrics. Before moving on to these sightly complex metrics let’s see what’s the problem…},
	language = {en},
	urldate = {2021-04-29},
	journal = {Medium},
	author = {Mittapally, Nitin},
	month = may,
	year = {2019},
}

@misc{noauthor_precision_2021,
	title = {Precision and recall},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Precision_and_recall&oldid=1014238124},
	abstract = {In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.
Suppose a computer program for recognizing dogs (the relevant element) in photographs identifies eight dogs in a picture containing ten cats and twelve dogs, and of the eight it identifies as dogs, five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / all positives) while its recall is 5/12 (true positives / relevant elements).
When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.
Adopting a hypothesis-testing approach from statistics, in which, in this case, the null hypothesis is that a given item is irrelevant, i.e., not a dog, absence of type I and type II errors (i.e. perfect specificity and sensitivity of 100\% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative).  
More generally, recall is simply the complement of the type II error rate, i.e., one minus the type II error rate. Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs an irrelevant item.
The above cat and dog example contained 8 − 5 = 3 type I errors, for a type I error rate of 3/10, and 12 − 5 = 7 type II errors, for a type II error rate of 7/12.  Precision can be seen as a measure of quality, and recall as a measure of quantity. 
Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).},
	language = {en},
	urldate = {2021-04-29},
	journal = {Wikipedia},
	month = mar,
	year = {2021},
	note = {Page Version ID: 1014238124},
}

@misc{wood_f-score_2019,
	title = {F-{Score}},
	url = {https://deepai.org/machine-learning-glossary-and-terms/f-score},
	abstract = {The F score, also called the F1 score or F measure, is a measure of a test’s accuracy.},
	urldate = {2021-04-29},
	journal = {DeepAI},
	author = {Wood, Thomas},
	month = may,
	year = {2019},
}

@misc{noauthor_precision_2020,
	title = {Precision vs {Recall} {\textbar} {Precision} and {Recall} {Machine} {Learning}},
	url = {https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/},
	abstract = {Precision vs recall are two crucial topics in machine learning. In this article learn about precision and recall in machine learning},
	urldate = {2021-04-29},
	journal = {Analytics Vidhya},
	month = sep,
	year = {2020},
}

@misc{shung_accuracy_2020,
	title = {Accuracy, {Precision}, {Recall} or {F1}?},
	url = {https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9},
	abstract = {Often when I talk to organizations that are looking to implement data science into their processes, they often ask the question, “How do I…},
	language = {en},
	urldate = {2021-04-29},
	journal = {Medium},
	author = {Shung, Koo Ping},
	month = apr,
	year = {2020},
}

@misc{rosebrock_intersection_2016,
	title = {Intersection over {Union} ({IoU}) for object detection},
	url = {https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/},
	abstract = {Discover how to apply the Intersection over Union metric (Python code included) to evaluate custom object detectors.},
	language = {en-US},
	urldate = {2021-04-29},
	journal = {PyImageSearch},
	author = {Rosebrock, Adrian},
	month = nov,
	year = {2016},
}

@misc{uavs_comparing_2019,
	title = {Comparing {Object} {Detection} {Algorithms}, {Demystified}},
	url = {https://amadoruavs.medium.com/comparing-object-detection-algorithms-demystified-cf26c9eb23ef},
	abstract = {(NOTE: This article is an attempt to explain detection metrics to beginners. For a more detailed and advanced article, check out Jonathan…},
	language = {en},
	urldate = {2021-04-29},
	journal = {Medium},
	author = {UAVs, Amador},
	month = jan,
	year = {2019},
}

@misc{sheremet_intersection_2020,
	title = {Intersection over union ({IoU}) calculation for evaluating an image segmentation model},
	url = {https://towardsdatascience.com/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686},
	abstract = {A practical example of calculating the IoU metric that allows us to evaluate how similar a predicted bounding box is to the ground truth…},
	language = {en},
	urldate = {2021-04-29},
	journal = {Medium},
	author = {Sheremet, Oleksii},
	month = sep,
	year = {2020},
}

@misc{noauthor_melodic_nodate,
	title = {melodic - {ROS} {Wiki}},
	url = {http://wiki.ros.org/melodic},
	urldate = {2021-04-23},
}

@misc{noauthor_python_nodate,
	title = {Python 2.7.0 {Release}},
	url = {https://www.python.org/download/releases/2.7/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2021-04-22},
	journal = {Python.org},
}

@misc{noauthor_mscthesis-generatedataforunseenobjects_nodate,
	title = {{MScThesis}-{GenerateDataForUnseenObjects}},
	url = {https://www.overleaf.com/project/5ffd813835be52ad40d70239},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2021-04-21},
}

@misc{noauthor_topics_nodate,
	title = {Topics - {ROS} {Wiki}},
	url = {http://wiki.ros.org/Topics},
	urldate = {2021-04-21},
}

@misc{noauthor_rostutorialsunderstandingnodes_nodate,
	title = {{ROS}/{Tutorials}/{UnderstandingNodes} - {ROS} {Wiki}},
	url = {http://wiki.ros.org/ROS/Tutorials/UnderstandingNodes},
	urldate = {2021-04-21},
}

@misc{noauthor_master_nodate,
	title = {Master - {ROS} {Wiki}},
	url = {http://wiki.ros.org/Master},
	urldate = {2021-04-21},
}

@misc{noauthor_robot_nodate,
	title = {Robot and interface specifications — {Franka} {Control} {Interface} ({FCI}) documentation},
	url = {https://frankaemika.github.io/docs/control_parameters.html},
	urldate = {2021-04-02},
}

@article{al-naimi_robotics_nodate,
	title = {Robotics and automation},
	language = {en},
	author = {Al-Naimi, Dr Ibrahim},
	pages = {33},
}

@article{zheng_mathematical_1985,
	title = {Mathematical modeling of a robot collision with its environment},
	volume = {2},
	issn = {07412223, 10974563},
	url = {http://doi.wiley.com/10.1002/rob.4620020307},
	doi = {10.1002/rob.4620020307},
	language = {en},
	number = {3},
	urldate = {2021-04-02},
	journal = {Journal of Robotic Systems},
	author = {Zheng, Yuan-Fang and Hemami, Hooshang},
	year = {1985},
	pages = {289--307},
}

@misc{noauthor_opencv_nodate,
	title = {{OpenCV}},
	url = {https://opencv.org/},
	abstract = {OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning (ML) and Artificial Intelligence (AI).},
	language = {en-US},
	urldate = {2021-04-01},
	journal = {OpenCV},
}

@article{quigley_ros_2009,
	title = {{ROS}: an open-source {Robot} {Operating} {System}},
	volume = {3},
	author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew},
	month = may,
	year = {2009},
	pages = {5},
}

@misc{noauthor_robotworx_nodate,
	title = {{RobotWorx} - {The} {Future} of {Automated} {Random} {Bin} {Picking}},
	url = {https://www.robots.com/blogs/the-future-of-automated-random-bin-picking},
	urldate = {2021-03-26},
}

@misc{noauthor_about_nodate,
	title = {About},
	url = {https://pointcloudlibrary.github.io/about/},
	abstract = {The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Point Cloud Library},
}

@misc{ambalina_5_2020,
	title = {5 {Computer} {Vision} {Companies} to {Follow} in 2020},
	url = {https://lionbridge.ai/articles/5-computer-vision-companies-to-follow-in-2020/},
	abstract = {From AI-powered security cameras to cancer detection and virtual reality, this list will cover 5 computer vision companies building the AI technology of tomorrow.},
	language = {en},
	urldate = {2021-03-19},
	journal = {Lionbridge AI},
	author = {Ambalina, Limarc},
	month = jan,
	year = {2020},
}

@misc{mishra_computer_2021,
	title = {Computer {Vision}: {Image} formation and representation},
	shorttitle = {Computer {Vision}},
	url = {https://towardsdatascience.com/computer-vision-image-formation-and-representation-a63e348e16b4},
	abstract = {Getting started with computer vision},
	language = {en},
	urldate = {2021-03-19},
	journal = {Medium},
	author = {Mishra, Mayank},
	month = jan,
	year = {2021},
}

@incollection{szeliski_introduction_2010,
	title = {Introduction},
	volume = {22},
	isbn = {978-1-84882-935-0},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
	language = {en},
	booktitle = {Computer {Vision}: {Algorithms} and {Applications}},
	publisher = {Springer Science \& Business Media},
	author = {Szeliski, Richard},
	month = sep,
	year = {2010},
	note = {Google-Books-ID: bXzAlkODwa8C},
	keywords = {Computers / Computer Graphics, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General},
}

@misc{mihajlovic_everything_nodate,
	title = {Everything {You} {Ever} {Wanted} {To} {Know} {About} {Computer} {Vision}. {\textbar} by {Ilija} {Mihajlovic} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e},
	urldate = {2021-02-24},
	author = {Mihajlovic, Ilija},
}

@article{wilson_relative_1996,
	title = {Relative end-effector control using {Cartesian} position based visual servoing},
	volume = {12},
	issn = {2374-958X},
	doi = {10.1109/70.538974},
	abstract = {This paper presents a complete design methodology for Cartesian position based visual servo control for robots with a single camera mounted at the end-effector. Position based visual servo control requires the explicit calculation of the relative position and orientation (POSE) of the workpiece object with respect to the camera. This is accomplished using image plane measurements of a number of known feature points on the object, and then applying an extended Kalman filter to obtain a recursive solution of the photogrammetric equations, and to properly combine redundant measurements. The control is then designed by specifying the desired trajectories with respect to the object and forming the control error in the end-effector frame. The implementation using a distributed computer architecture is described. An experimental system has been built and used to evaluate the performance of the POSE estimation and the position based visual servo control. Several results for relative trajectory control and target tracking are presented. Results of the experiments showing the effect of loss of some of the redundant features are also presented.},
	number = {5},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Wilson, W. J. and Hulls, C. C. Williams and Bell, G. S.},
	month = oct,
	year = {1996},
	note = {Conference Name: IEEE Transactions on Robotics and Automation},
	keywords = {Cameras, Cartesian position control, Computer architecture, Computer errors, Design methodology, Equations, Error correction, Kalman filter, Kalman filters, POSE estimation, Robot vision systems, Servosystems, Target tracking, Trajectory, feature extraction, manipulator dynamics, optical tracking, photogrammetric equations, position control, relative end-effector control, relative orientation, relative position, robot vision, servomechanisms, target tracking, trajectory control, visual servo control},
	pages = {684--696},
}

@misc{noauthor_inventor_nodate,
	title = {Inventor {\textbar} {Mechanical} {Design} \& {3D} {CAD} {Software} {\textbar} {Autodesk}},
	url = {https://www.autodesk.com/products/inventor/overview, https://www.autodesk.com/products/inventor/overview},
	abstract = {Inventor 3D CAD software is used for product design, rendering, and simulation. Get professional-grade 3D CAD software for product design and engineering.},
	language = {en-US},
	urldate = {2021-03-08},
}

@misc{wallkotter_franka_2020,
	title = {Franka {Emika} {Panda} in {Gazebo} with {ROS} and {Docker}},
	url = {https://sebastianwallkoetter.wordpress.com/2020/03/08/franka-emika-panda-in-gazebo-with-ros-and-docker/},
	abstract = {In my last post I’ve written about creating Gazebo packages from the existing panda models and mentioned that I’m also working on a docker image that includes ROS. Well here it is :) Yo…},
	language = {en},
	urldate = {2021-03-03},
	journal = {Sebastian Wallkötter},
	author = {Wallkötter, Sebastian},
	month = mar,
	year = {2020},
}

@misc{truebenbach_is_2019,
	title = {Is fully automated bin picking finally here?},
	url = {https://www.therobotreport.com/fully-automated-bin-picking-finally-here/},
	abstract = {Universal bin picking needs to be usable by non-experts, quickly configurable, provide path planning with little or no tuning and be cost-effective.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {The Robot Report},
	author = {Truebenbach, Eric},
	month = feb,
	year = {2019},
}

@article{pickett_dont_2018,
	title = {Don't {Fear} the {Cobot}},
	copyright = {Copyright BNP Media Jan 2018},
	issn = {03609936},
	url = {https://search.proquest.com/docview/2052783973/abstract/5B7643FF906E4197PQ/1},
	abstract = {In an interview, robots experts Jeff Burnstein and Kyle Kidwell, talked about some of the advancements, pain points, and opportunities in the current state and not-so-distant future of the robotics industry. Burnstein said SIASUN and other leading Chinese robotics companies have robots for many applications, such as welding, material handling, and assembly. Several, including SIASUN, are also targeting service robots. I recently saw SIASUN products that could be used in elder care, an exciting development given the great need for advances in this area. Kidwell, said also some of the headaches in robot applications are setup and deployment. The typical setup time of a UR robot is around half a day, which is much shorter than most robot applications, as many of them require a lot of work with setting up peripherals and getting them running. He added that Financially speaking, robots have started to emphasize a quick ROI in the manufacturing industry.},
	language = {English},
	urldate = {2021-03-05},
	journal = {Quality},
	author = {Pickett, Leah},
	month = jan,
	year = {2018},
	note = {Num Pages: 12-15
Place: Troy, United States
Publisher: BNP Media
Section: VISION \& SENSORS: VISION ROBOTICS},
	keywords = {Automation, Business And Economics--Management, Engineering, Experts, Manufacturing, Manufacturing engineering, Materials handling, Metrology And Standardization, Robotics, Robots, Service robots, Technological change},
	pages = {12--15},
}

@article{accorsi_application_2019,
	series = {29th {International} {Conference} on {Flexible} {Automation} and {Intelligent} {Manufacturing} ( {FAIM} 2019), {June} 24-28, 2019, {Limerick}, {Ireland}, {Beyond} {Industry} 4.0: {Industrial} {Advances}, {Engineering} {Education} and {Intelligent} {Manufacturing}},
	title = {An application of collaborative robots in a food production facility},
	volume = {38},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978920300457},
	doi = {10.1016/j.promfg.2020.01.044},
	abstract = {Despite the food industry being a leading sector of the European economy, the level of penetration of automation is still low. The main reasons lie on the small margin of food items which does not encourage technological investments, the extremely spread vendors market i.e. mostly small and medium enterprises, and the high level of flexibility and care required to handle food products along production, packaging, and storage operations. Nevertheless, the advent of collaborative, small and flexible robots provides great opportunities for the design and development of new effective processes integrating the human flexibility with the efficiency of automation. This paper explores the impact of adopting collaborative robots in the food catering industry, by illustrating a case study developed for the end-of-line of a catering production system. A generalizable methodology is proposed to support the study of the technical and economic feasibility of the implementation of such technology. This methodology is intended to support managers of the food industry to analyse the constraints that limit the automation of a process and to measure the expected performance of the system in terms of throughput, ergonomics and economic benefits resulting from the adoption of collaborative robots.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Procedia Manufacturing},
	author = {Accorsi, R. and Tufano, A. and Gallo, A. and Galizia, F. G. and Cocchi, G. and Ronzoni, M. and Abbate, A. and Manzini, R.},
	month = jan,
	year = {2019},
	keywords = {Collaborative robots, Feasibility analysis, Food industry, Industry 4.0, Production automatization},
	pages = {341--348},
}

@article{schmidbauer_teaching_2020,
	series = {Learning {Factories} across the value chain – from innovation to service – {The} 10th {Conference} on {Learning} {Factories} 2020},
	title = {Teaching {Cobots} in {Learning} {Factories} – {User} and {Usability}-{Driven} {Implications}},
	volume = {45},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978920310817},
	doi = {10.1016/j.promfg.2020.04.043},
	abstract = {Up to now, industrial robots were considered as machines working for humans. In this sense, programming required special coding knowledge and skills as teaching approaches were based on imperative programming methods, formulating the solving of a problem line by line. As novel work principles consider the robot rather a tool, assisting or collaborating with the human, declarative approaches are needed, that allow for intuitiveness and modifiability. Collaborative robot (cobot) control requires intuitive interfaces, not only for ease of use but also for modifying existing execution programs. Furthermore, the increasingly diverse personnel also requires a more democratic approach to robot programming. However, there is no standard or guideline for intuitive cobot control, and it can be noticed that the usability of the diverse interfaces and systems provided on the market is rather poor. This paper compares three systems with their advantages and disadvantages concerning usability and presents the results of the standard usability score (SUS) test, which was conducted in the Vienna learning factory “TU Wien Pilot Factory for Industry 4.0”. Additionally, the paper presents an approach on how to teach different levels of cobot interaction and control addressing the skill sets needed in their individual working environments.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Procedia Manufacturing},
	author = {Schmidbauer, Christina and Komenda, Titanilla and Schlund, Sebastian},
	month = jan,
	year = {2020},
	keywords = {Cobot Teaching, Collaborative Robots, Learning Concept, Robot Programming, Usability},
	pages = {398--404},
}

@article{el_zaatari_cobot_2019,
	title = {Cobot programming for collaborative industrial tasks: {An} overview},
	volume = {116},
	issn = {0921-8890},
	shorttitle = {Cobot programming for collaborative industrial tasks},
	url = {https://www.sciencedirect.com/science/article/pii/S092188901830602X},
	doi = {10.1016/j.robot.2019.03.003},
	abstract = {Collaborative robots (cobots) have been increasingly adopted in industries to facilitate human–robot collaboration. Despite this, it is challenging to program cobots for collaborative industrial tasks as the programming has two distinct elements that are difficult to implement: (1) an intuitive element to ensure that the operations of a cobot can be composed or altered dynamically by an operator, and (2) a human-aware element to support cobots in producing flexible and adaptive behaviours dependent on human partners. In this area, some research works have been carried out recently, but there is a lack of a systematic summary on the subject. In this paper, an overview of collaborative industrial scenarios and programming requirements for cobots to implement effective collaboration is given. Then, detailed reviews on cobot programming, which are categorised into communication, optimisation, and learning, are conducted. Additionally, a significant gap between cobot programming implemented in industry and in research is identified, and research that works towards bridging this gap is pinpointed. Finally, the future directions of cobots for industrial collaborative scenarios are outlined, including potential points of extension and improvement.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Robotics and Autonomous Systems},
	author = {El Zaatari, Shirine and Marei, Mohamed and Li, Weidong and Usman, Zahid},
	month = jun,
	year = {2019},
	keywords = {Cobot, Human-awareness, Human–robot collaboration, Intuitive programming},
	pages = {162--180},
}

@inproceedings{redmon_you_2016-1,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	urldate = {2021-03-04},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
}

@misc{noauthor_machine_nodate,
	title = {Machine loading},
	url = {https://wiredworkers.io/applications/machine-loading/},
	abstract = {Cobots can be used to take over tasks from employees. One of these dangerous and time-consuming tasks is loading production machines.},
	language = {en-US},
	urldate = {2021-03-02},
	journal = {WiredWorkers},
}

@article{gaz_dynamic_2019,
	title = {Dynamic {Identification} of the {Franka} {Emika} {Panda} {Robot} {With} {Retrieval} of {Feasible} {Parameters} {Using} {Penalty}-{Based} {Optimization}},
	doi = {10.1109/LRA.2019.2931248},
	abstract = {In this letter, we address the problem of extracting a feasible set of dynamic parameters characterizing the dynamics of a robot manipulator. We start by identifying through an ordinary least squares approach the dynamic coefficients that linearly parametrize the model. From these, we retrieve a set of feasible link parameters (mass, position of center of mass, inertia) that is fundamental for more realistic dynamic simulations or when implementing in real time robot control laws using recursive Newton-Euler algorithms. The resulting problem is solved by means of an optimization method that incorporates constraints on the physical consistency of the dynamic parameters, including the triangle inequality of the link inertia tensors as well as other user-defined, possibly nonlinear constraints. The approach is developed for the increasingly popular Panda robot by Franka Emika, identifying for the first time its dynamic coefficients, an accurate joint friction model, and a set of feasible dynamic parameters. Validation of the identified dynamic model and of the retrieved feasible parameters is presented for the inverse dynamics problem using, respectively, a Lagrangian approach and Newton-Euler computations.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gaz, Claudio and Cognetti, Marco and Oliva, A. and Giordano, Paolo Robuffo and Luca, A. De},
	year = {2019},
}

@misc{noauthor_opencv_nodate-1,
	title = {{OpenCV}: {Contours} : {Getting} {Started}},
	url = {https://docs.opencv.org/master/d4/d73/tutorial_py_contours_begin.html},
	urldate = {2021-02-25},
}

@misc{noauthor_yolo_nodate,
	title = {{YOLO}: {Real}-{Time} {Object} {Detection}},
	url = {https://pjreddie.com/darknet/yolo/},
	urldate = {2021-02-25},
}

@misc{gmbh_franka_nodate,
	title = {Franka {Emika}},
	url = {https://www.franka.de/technology},
	abstract = {The robot for everyone – sensitive, interconnected, adaptive and cost-efficient.},
	language = {en-US},
	urldate = {2021-02-24},
	author = {GmbH, Franka Emika},
}

@misc{noauthor_c_nodate,
	title = {C (programming language) - encyclopedia article - {Citizendium}},
	url = {https://en.citizendium.org/wiki/C_(programming_language)},
	urldate = {2021-02-24},
}

@misc{noauthor_what_nodate-1,
	title = {What is {Python}? {Executive} {Summary}},
	shorttitle = {What is {Python}?},
	url = {https://www.python.org/doc/essays/blurb/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2021-02-24},
	journal = {Python.org},
}

@misc{noauthor_ubuntu_nodate,
	title = {Ubuntu release cycle},
	url = {https://ubuntu.com/about/release-cycle},
	abstract = {Overview of the Ubuntu release cycle - maintenance, support and security coverage, lifetime, upgrade paths, kernel versions and the range of editions and images published by Canonical.},
	language = {en},
	urldate = {2021-02-24},
	journal = {Ubuntu},
}

@misc{noauthor_lightnet_nodate,
	title = {Lightnet documentation — {Lightnet} 2.0.1 documentation},
	url = {https://eavise.gitlab.io/lightnet/},
	urldate = {2021-02-24},
}

@misc{noauthor_ultralyticsyolov3_2021,
	title = {ultralytics/yolov3},
	copyright = {GPL-3.0 License         ,                 GPL-3.0 License},
	url = {https://github.com/ultralytics/yolov3},
	abstract = {YOLOv3 in PyTorch {\textgreater} ONNX {\textgreater} CoreML {\textgreater} TFLite. Contribute to ultralytics/yolov3 development by creating an account on GitHub.},
	urldate = {2021-02-24},
	publisher = {Ultralytics LLC},
	month = feb,
	year = {2021},
	note = {original-date: 2018-08-26T08:57:20Z},
	keywords = {coreml, deep-learning, ios, machine-learning, ml, object-detection, onnx, pytorch, tflite, yolo, yolov3, yolov4, yolov5},
}

@misc{noauthor_darknet_nodate,
	title = {Darknet: {Open} {Source} {Neural} {Networks} in {C}},
	url = {https://pjreddie.com/darknet/},
	urldate = {2021-02-24},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding darknet's yolo.cfg config files},
	url = {https://stackoverflow.com/questions/50390836/understanding-darknets-yolo-cfg-config-files},
	urldate = {2021-02-24},
	journal = {Stack Overflow},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2021-02-24},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{brooks_jsbrokscoco-annotator_2021,
	title = {jsbroks/coco-annotator},
	copyright = {MIT License         ,                 MIT License},
	url = {https://github.com/jsbroks/coco-annotator},
	abstract = {:pencil2: Web-based image segmentation tool for object detection, localization, and keypoints},
	urldate = {2021-02-24},
	author = {Brooks, Justin},
	month = feb,
	year = {2021},
	note = {original-date: 2018-09-03T21:38:31Z},
	keywords = {annotate-images, coco, coco-annotator, coco-format, computer-vision, datasets, deep-learning, detection, image-annotation, image-labeling, image-segmentation, label, machine-learning},
}

@inproceedings{rahimi_industrial_2017,
	title = {An {Industrial} {Robotics} {Application} with {Cloud} {Computing} and {High}-{Speed} {Networking}},
	doi = {10.1109/IRC.2017.39},
	abstract = {This paper describes an industrial cloud robotics distributed application that was executed across a high-speed wide-area network. The application was implemented using ROS libraries and packages. The purpose of the application is to enable an industrial robot to perform surface blending. A Kinect sensor, a surface blending tool and a laser scanner are mounted on the robot arm. The arm is moved under software control to scan a work bench on which metal parts of variable size can be laid out at any orientation. The collected point cloud data is processed by a segmentation algorithm to find the surface boundaries. A Cartesian path planning algorithm is executed to determine paths for the robot arm to execute the blending action and a laser scan on a selected surface. A new ROS package was implemented to collect CPU, memory and bandwidth usage for each significant ROS node in this distributed application. To emulate a scenario in which computing resources at a remote datacenter can be used for the segmentation and path planning algorithms in conjunction with the robots located on a factory floor, a software-defined network testbed called GENI was used to distribute compute-heavy ROS nodes. Measurements show that with TCP tuning, and high-speed end-to-end paths, the total execution time in the Cloud scenario can be reasonably close to a local scenario in which computing is collocated with the robot.},
	booktitle = {2017 {First} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Rahimi, R. and Shao, C. and Veeraraghavan, M. and Fumagalli, A. and Nicho, J. and Meyer, J. and Edwards, S. and Flannigan, C. and Evans, P.},
	month = apr,
	year = {2017},
	keywords = {Cartesian path planning algorithm, Cloud computing, GENI, Global Environment for Network Innovations, High-speed networks, Industrial robotics, Kinect sensor, Manipulators, Metals, ROS, ROS libraries, ROS packages, ROS-I, Robot sensing systems, Service robots, Surface treatment, TCP tuning, cloud computing, control engineering computing, high-speed networking, image segmentation, industrial robotics application, industrial robots, laser scanner, manipulators, operating systems (computers), path planning, production engineering computing, robot arm, segmentation algorithm, software defined networking, software-defined network testbed, surface blending tool, surface boundaries, wide area networks, wide-area network},
	pages = {44--51},
}

@article{chitta_moveitros_2012,
	title = {Moveit![{ROS} topics]},
	volume = {19},
	doi = {10.1109/MRA.2011.2181749},
	journal = {IEEE Robotics \& Automation Magazine - IEEE ROBOT AUTOMAT},
	author = {Chitta, Sachin and Sucan, Ioan and Cousins, Steve},
	month = mar,
	year = {2012},
	pages = {18--19},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-02-24},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
}

@misc{noauthor_ros_nodate,
	title = {{ROS} {Index}},
	url = {https://index.ros.org/packages/},
	urldate = {2021-02-24},
}

@misc{noauthor_rosorg_nodate,
	title = {{ROS}.org {\textbar} {Is} {ROS} {For} {Me}?},
	url = {https://www.ros.org/is-ros-for-me/},
	language = {en-US},
	urldate = {2021-02-24},
}

@misc{noauthor_robotics_nodate,
	title = {Robotics {Industry} {Insights} - {Robotic} {Bin} {Picking} – {The}...},
	url = {https://www.robotics.org/content-detail.cfm/Industrial-Robotics-Industry-Insights/Robotic-Bin-Picking-The-Holy-Grail-in-Sight/content_id/6002},
	urldate = {2021-02-24},
}

@book{buchholz_bin-picking_2015,
	title = {Bin-{Picking}: {New} {Approaches} for a {Classical} {Problem}},
	isbn = {978-3-319-26500-1},
	shorttitle = {Bin-{Picking}},
	abstract = {This book is devoted to one of the most famous examples of automation handling tasks – the “bin-picking” problem. To pick up objects, scrambled in a box is an easy task for humans, but its automation is very complex. In this book three different approaches to solve the bin-picking problem are described, showing how modern sensors can be used for efficient bin-picking as well as how classic sensor concepts can be applied for novel bin-picking techniques. 3D point clouds are firstly used as basis, employing the known Random Sample Matching algorithm paired with a very efficient depth map based collision avoidance mechanism resulting in a very robust bin-picking approach. Reducing the complexity of the sensor data, all computations are then done on depth maps. This allows the use of 2D image analysis techniques to fulfill the tasks and results in real time data analysis. Combined with force/torque and acceleration sensors, a near time optimal bin-picking system emerges. Lastly, surface normal maps are employed as a basis for pose estimation. In contrast to known approaches, the normal maps are not used for 3D data computation but directly for the object localization problem, enabling the application of a new class of sensors for bin-picking.},
	language = {en},
	publisher = {Springer},
	author = {Buchholz, Dirk},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: BfYUCwAAQBAJ},
	keywords = {Computers / Computer Graphics, Computers / Intelligence (AI) \& Semantics, Computers / Optical Data Processing, Technology \& Engineering / Engineering (General), Technology \& Engineering / General, Technology \& Engineering / Robotics},
}

@misc{noauthor_working_2017,
	title = {Working with {Images} in {Python}},
	url = {https://www.geeksforgeeks.org/working-images-python/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {GeeksforGeeks},
	month = jan,
	year = {2017},
	note = {Section: Python},
}

@misc{noauthor_intel_nodate,
	title = {Intel® {RealSense}™ {LiDAR} {Camera} {L515}},
	url = {https://www.intelrealsense.com/lidar-camera-l515/},
	abstract = {World's smallest high-resolution LiDAR camera from Intel®. Low power and low weight, open-source SDK.},
	language = {en-US},
	urldate = {2021-02-24},
	journal = {Intel® RealSense™ Depth and Tracking Cameras},
}
