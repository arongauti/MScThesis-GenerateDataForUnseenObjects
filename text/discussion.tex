\chapter{Discussion}
This project set out to look into the possibility of using an robot manipulator to generate training data for previously unseen objects. But in order to reach that conclusion, objectives of the project had to be divided into concise sections, which included three research questions which the project aimed to answer. These research questions were as follows: i) Is it possible to generate new arrangements of objects using a robot manipulator? ii) Is it possible to annotate objects automatically, determining the extent of the objects in the images? iii) Is it possible to improve the performance of a Convolutional Neural Network using automatically generated training data from robot?

%\lipsum[35-41]
\section{Using a robot to create image data set}
%It was challenging to get the robot to work as he should and learn how to program it. 
From the tests in \textit{Section \ref{resrobotcontrol}} the robot would save time on creating a dataset since it only takes the robot on average 12.64 seconds (\textit{From Table\ref{tab:testonrobot}}) to move and rotate one object. Comparing to an human it would take around 30 seconds to move it, rotate it and capture an image. The robot managed to finish both the test by moving both the objects 100 times and 300 times without stopping. That tells us that it is possible to use robot to move the object to the other side. 

Results also showed that the Nivea Cleansing Milk bottle needed more often operator intervention or 2\% more often. The reason for that is the Nivea bottle is both smaller and more curved than the Alberto Balsam bottle, so it makes it harder for the robot to pick it successfully every time. The main reason for these failures is how the program is coded. Since the first version of the robot program was designed in that way that the neural network would only send the robot one coordinate in the first iteration, and then the robot would move the object to an mirrored (negative) coordinate. When the robot iterates often it can create an error when it drops the object. That is not good since the robot has only fixed coordinates of the object (both mirrored and not).

It would have been an possibility to create different version of the robot program and make it always get coordinates from the neural network, but then it would not been possible to find the object if it was rotated. But since the first version was used to create an 376 image dataset it would be possible to create an program that will always use the trained neural network since it has been trained on the items in first dataset in all orientations.


\section{Automatic labelling}
\subsection{Before vs. after}
The initial hypothesis was that objects could be annotated by comparing two images, where one object had been moved in between images. Since it should be easy to see the difference, but it is not that simple. This method is described in \textit{Section \ref{subsec:beforeafter}}.

Sometimes objects intersect in the before- and after image. In that cases when the bottles area intersect each other it can't find the right bounding box and that are not good results, the results can been seen in \textit{Figure \ref{figure: imagework2}}. Therefor this method would not work so another method would be recommended to find the right bounding box automatically. 

\subsection{Empty bin vs. Object in the bin}
The second method \textit{(Section \ref{subsec:emptybin})} showed an increase in performance and almost perfect results since it needed to go through two functions of object finding. It works for dark items, light items or strange shaped items. This method uses two function, one function returns almost right bounding box every time since it finds the contours of the item. But when the objects have strange colors, such as a bottle with both white and black the results are not that good, and that is why there are two functions so there will be no bad results. But the second function finds the difference between an empty box vs. an item in the box.

This method \textit{(Section \ref{subsec:emptybin})} returned almost perfect visual results, as it tested how long it would take the robot computer to annotate the all of the images. \textit{Table \ref{tab:timediff}} shows the average time which is 0.85 seconds per image, that is not much compared to how long it takes a human to annotate. In \textit{Section \ref{sec:beiersdorfdataset}} it can be seen that it takes human on average 5 minutes to annotate one image.

\section{Training neural networks} 
\subsection{Training the first neural network}
The results from the first trained neural network are promising. \textit{Figure \ref{fig:neuralnetwork}} show how the IoU developed over number of times the training set has been presented to the network. Also it shows that it has the best model for the test set at 38000 epochs. This tells us that the neural network model at 38000 epochs would give us the best results on the training and test set. But it is not necessary the best model for unknown/untrained products.

\textit{Figure \ref{figure: beforeaftertraining}} show how the results are visually and how the performance is on the first trained neuaral network. That results shows us that the trained neural network shows considerably better object detection than the YOLOv4. The trained neural network return around 98\% confident on known items, but sometimes it cuts a piece of the bottles. It always returns center point that is able to be picked which is what this project is about. 


\subsubsection{Single known items}
%  34 50 71 73 83 93
\textit{Section \ref{sec:resontrained}} show how the trained neural network performs on previously unseen images of known products included in the training set. \textit{Figure \ref{figure: knownproducts}} shows how the intersection over union behaves over all of the images. Overall the average IoU is good for all of these 4 products but the Alberto Balsam has 6 outliers points.% The reason for that is there were 6 automatic labelled images that didn't have good annotation and they were images 34, 49, 70, 72, 82 and 92 of the Alberto Balsam bottles.

Results in \textit{Table \ref{tab:ready}} show that the trained neural network has an 100\% True Positive rate, which means that the detection test always finds the right item. The average IoU for known products is 92\% which tells us that the neural network was trained right.

\textit{Figure \ref{fig:boxknownproducts}} show how the results from the detection run summarize in a box plot. Nivea Cleansing Milk has the highest IoU value, and the Alberto Balsam has the lowest value. As mentioned before the Alberto Balsam has 6 outliers, which is shown on the box plot.

\subsubsection{On unknown Beiersdorf products}
In \textit{Section \ref{subsec:resunknownprod}} it can be seen how the trained neural network performs on images containing multiple unknown items from the Beiersdorf dataset. \textit{Figure \ref{figure: unknownproducts}} show how the intersection over union behaves over the unknown products, the average IoU is not as good for the unknown products. The highest average IoU is on the item nr. 11 and the reason for that is that the item is similar to the items that were trained. The lowest average IoU is on the item nr. 6. The reason for that is the item has a cylindrical shape which is not similar to the items which the network were trained on.  

\textit{Table \ref{tab:test1unknown}} shows how the trained neural networks worked on the unknown products. It shows that item nr. 11 and nr. 12 has the best results. Item 11 has the best IoU but item 12 has better F-score or 80\%. There are two items that have extremely bad results, those items are the two cylindrically shaped items, item nr. 5 and item nr. 6. From that results it is possible to compare these results to the results on the known items. The average IoU for known items was 92\% compared to an average IoU of 60\% when working on unknown items. That would possible mean that the neural network would need better training images. 

In \textit{Figure \ref{fig:unknowniou}} is a box plot for each product that provides a visual summary of the results in \textit{Table \ref{tab:test1unknown}}. In the \textit{Figure \ref{fig:unknownioua}} it can be seen that the neural network works least well on item 5 and 6. In the \textit{Figure \ref{fig:unknownioub}} it can be seen that the neural network works least on item 5, 6 and 14, and it strange that item 14 or better known as Alberto Balsam gives low F-score since that is the only bottle that was in the first dataset and was also trained with the neural network. The reason for these bad results on the Alberto Balsam bottle, is that those images are taken from a greater distance from the bin, so there are some objects in the background that interrupt the network. 

\textit{Figure \ref{fig:bottles}} shows how the trained neural network performed for different number of items in the bin. In this box plot the X-axis is the number of bottles in the bin, then it could be seen how the IoU changes while the number of bottles increases in the bin. It can be seen that the best IoU is when there are only one bottle in the bin, and decreases when bottle are added in the bin. This is to be expected as the first network model was only trained on images with one object in the bin and always had an gray background, so when that color changed the network got into a problem.


\subsection{Training the second neural network}
\subsection{Training the third neural network}
\section{Summary}
%\lipsum[42-43]

\section{Conclusion\label{sec:conclusions}}

%\lipsum[44-50]
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "DEGREE-NAME-YEAR"
%%% End: 
