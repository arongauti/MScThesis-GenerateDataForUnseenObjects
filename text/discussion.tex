\chapter{Discussion}

%\lipsum[35-41]
\section{Using a robot to create image data set}
%It was challenging to get the robot to work as he should and learn how to program it. 
From the tests in \textit{Section \ref{resrobotcontrol}} the robot would save time on creating a dataset as it only takes the robot on average 12.64 seconds (\textit{From Table \ref{tab:testonrobot}}) to move and rotate one object. Comparing to a human it would take around 30 seconds to move, rotate and capture an image. The robot managed to finish both tests by moving both objects 100 times and 300 times without stopping. Meaning that it is possible to use a robot to move the object to the other side. 

Results also showed that the Nivea Cleansing Milk bottle needed more often operator intervention or 2\% compared to Alberto Balsam which only needed 0.25\% intervention. The reason for that is because the Nivea bottle is both smaller and more curved than the Alberto Balsam bottle, so it makes it harder for the robot to pick it successfully every time. The main reason for this failure is how the program is coded. Since the first version of the robot program was designed so the neural network would only send the robot one coordinate in the first iteration, and then the robot would move the object to a mirrored (negative) coordinate. When the robot iterates often it can create an error when it drops the object. That is not good since the robot has only fixed coordinates of the object (both mirrored and not).

It would have been a possibility to create a different version of the robot program and make it always get coordinates from the neural network, but then the robot would not be able to find the object if it was rotated. But since the first version was used to create a 376 image dataset it would be possible to create a program that will always use the trained neural network since it has been trained on the items in the first dataset in all orientations.

When the first two neural networks finished training it was possible to create a new version of the robot software and make it use the second neural network to find objects in the bin and remove them from the bin. Then it was possible to capture images (before and after) and create new a dataset with multiple items in the bin.

\section{Automatic labelling}
\subsection{Before vs. after}
The initial hypothesis was that objects could be annotated by comparing two images, where one object had been moved between images. It should be easy to see the difference, but it is not that simple. This method is described in \textit{Section \ref{subsec:beforeafter}}.

Sometimes objects intersect in the before- and after image. In those cases when the area of the bottle intersect each other the robot can't find the right bounding box, which does not provide good results, as showcased in \textit{Figure \ref{figure: imagework1}} and \textit{Figure \ref{figure: imagework2}}. This method would therefore not work when there is only one object in the bin unless the robot knows the object's shape. So, another method would be recommended to find the right bounding box automatically. 
However, this method would work if the bin were full of objects, the robot would then pick one object up and move it to another bin. \textit{Figure \ref{figure: multiimagework1}} and \textit{Figure \ref{figure: multiimagework2}} show an example of good and bad automatic annotation using this method. The reason for bad annotation is that two objects moved and not only one. The bad annotations often have contours noise in the image, and it is possible to get rid of the noise by taking the average over the contours and remove contour outlier’s points. Also, it would be possible to not use images that have a bounding box larger than certain max value. 

\subsection{Empty bin vs. Object in the bin}
The second method \textit{(Section \ref{subsec:emptybin})} finds a difference between an image of an empty bin and an image with an object in the bin. 
This method showed an increase in performance and almost perfect results since it had to go through two functions of object finding. It works for dark items, light items, and strangely shaped items. 
%\fxfatal{This method uses two functions, one function returns almost the right bounding box every time since it finds the contours of the item. But when the objects have strange colors, such as a bottle with both white and black the results are not that good, and that is why there are two functions so there will be no bad results. But the second function finds the difference between an empty box vs. an item in the box.SKOÐA comment}
%These details of the methods should be described in the method section (or even background). The reader can then be referred to this description in the discussion. Examples of "strange color, such as a bottle with both white and black" should be presented in results, but the implications are discussed here. New results should not be presented here.

This method \textit{(Section \ref{subsec:emptybin})} returned almost perfect visual results, as it tested how long it would take the robot computer to annotate all of the images. Some annotations were not perfect as shown in \textit{Figure \ref{figure: labelling}} and \textit{Figure \ref{figure: badlabelling}}, they show an examples of successful and failed boundary box detection.
\textit{Table \ref{tab:timediff}} shows the average time which is 0.85 seconds per image, which is not much compared to how long it takes a human to annotate. In \textit{Section \ref{sec:beiersdorfdataset}} it can be seen that it takes human on average 5 minutes to annotate one image. \textit{Table \ref{tab:annotation}} shows the annotation performance over the first dataset which had only one object in the bin, it had a good annotation in 98\% cases.



\section{Training neural networks} 
\subsection{Training the first neural network}
The results from the first trained neural network are promising. \textit{Figure \ref{fig:neuralnetwork}} show how the IoU developed over the number of times the training set has been presented to the network. Also, it shows that it has the best model for the test set at 38000 epochs. This indicates that the neural network model at 38000 epochs would give the best results on the training and test set. However, it is not necessarily the best model for unknown/untrained products.

\textit{Figure \ref{figure: beforeaftertraining}} show the results visually and how the performance is on the first trained neural network. The trained neural network show considerably better object detection than the prior model which was trained on the COCO dataset. The trained neural network return around 98\% confident on known items, but sometimes it cuts a piece of the bottles. However, it always returns a center point that can be picked which is what this project is about. 


\subsubsection{Single known items}
% 34 50 71 73 83 93
\textit{Section \ref{sec:resontrained}} show how the trained neural network performs on previously unseen images of known products included in the training set. \textit{Figure \ref{figure: knownproducts}} shows how the IoU behaves over all of the images. Overall, the average IoU is good for all of these 4 products, but the Alberto Balsam has 6 outliers points. % The reason for that is 6 automatic labelled images didn't have good annotation and they were images 34, 49, 70, 72, 82, and 92 of the Alberto Balsam bottles.

Results in \textit{Table \ref{tab:ready}} show that the trained neural network has a 100\% True Positive rate, which means that the detection test always finds the right item. The average IoU for known products is 96.0\% meaning that the neural network was trained right.

\textit{Figure \ref{fig:boxknownproducts}} shows how the results from the detection run summarized in a box plot. Nivea Cleansing Milk has the highest IoU value, and Alberto Balsam has the lowest value. As mentioned before the Alberto Balsam has 6 outliers, which is shown on the box plot.

\textit{Figure \ref{figure: v1bestworst}} shows the highest and the lowest IoU score on the single known items when using the first neural network. The lowest IoU is on the image that was worst annotated as is shown in \textit{Figure \ref{figure: v1worst}}. The neural network finds a good bounding box (red), but it does not match with the automatically annotated bounding box (green).

\subsubsection{On unknown Beiersdorf products}
\textit{Section \ref{subsec:resunknownprod}} shows how the trained neural network performs on images containing multiple unknown items from the Beiersdorf dataset. \textit{Figure \ref{figure: unknownproducts}} show how the IoU behaves over the unknown products, the IoU average is not as good for the unknown products, because it was not trained on that dataset. The highest IoU average is on item nr. 11 and the reason for that is that the item is like the items that were trained. The lowest IoU average is on item nr. 6. The reason for that is that the item has a cylindrical shape, which differs from the items used to train the network.

\textit{Table \ref{tab:test1unknown}} shows how the trained neural networks worked on the unknown products. It shows that item nr. 11 and nr. 12 has the best results. Item 11 has the best IoU but item 12 has a better F-score or 83.5\%. The model has the most trouble with the cylindrical-shaped items, nr. 5 and nr. 6. It is possible to compare these results to the results on the known items. The IoU average for known items was 96.0\% compared to an average IoU of 63.2\% when working on unknown items. That would possibly mean that the neural network would need better training images. 

In \textit{Figure \ref{fig:unknowniou}} is a box plot for each product that provides a visual summary of the results in \textit{Table \ref{tab:test1unknown}}. In the \textit{Figure \ref{fig:unknownioua}} it can be seen that the neural network works least well on items 5 and 6. \textit{Figure \ref{fig:unknownioub}} shows that the first neural network works least on item 5, 6, and 14. It is though strange that item 14 or better known as Alberto Balsam gives a low F-score because that was the only bottle that was in the first dataset and was also trained with the neural network. The reason for these bad results on the Alberto Balsam bottle is because those images are taken from a greater distance from the bin, so there are some objects in the background that interrupt the network. 

\textit{Figure \ref{fig:bottles}} shows how the trained neural network performed for a different number of items in the bin. In this box plot, the best IoU is when there is only one bottle in the bin, and the IoU decreases when a bottle is added to the bin. This is to be expected as the first network model was only trained on images with one object in the bin and always had a gray background. However, when that color changed the network got into a problem.
\textit{Figure \ref{fig:v1unknowniou}} and \textit{Figure \ref{fig:v1max}} show visually how the first neural network performs on the Beiersdorf dataset, it shows the highest and lowest IoU and also the highest TP and highest FP. The highest IoU was on item nr. 11 with the IoU score of 97.6\%. The highest TP was on item nr. 5 or 7 TP, which is strange since that item has one of the lowest IoU scores in \textit{Table \ref{tab:test1unknown}}.

%%%%%%%%%%%%%%%%%%%%%%%%% ----------------- Second neural network ------------------------------------
\subsection{Training the second neural network}
%\fxfatal{Klára að skrifa}
\textit{Figure \ref{fig:v2neuralnetwork}} show how the IoU developed over a number of times the training set has been presented to the network. Also, it shows that it has the best model for the test set at 37000 epochs. This tells us that the neural network model at 37000 epochs would give us the best results on the training and test set. But it is not necessary the best model for unknown/untrained products.

\textit{Figure \ref{figure: v2beforeaftertraining}} show how the results are visually and how the performance is on the second trained neural network. These results show us that the trained neural network shows considerably better object detection than the prior model which was trained on the COCO dataset. The results from the \textit{Section \ref{sec:secondneural}} show that the second neural network shows similar results to the first neural network ({Section \ref{sec:firstneural}}). This shows that not all images need to be annotated 100\% correct, as mentioned before the first neural network had only good annotations and the second neural network had both bad and good annotations. The results show that a fully automatically annotated dataset can be used to train a neural network and the second neural network can be used to train further on.


\subsubsection{Single known items}
\textit{Section \ref{sec:v2resontrained}} show how the trained neural network performs on previously unseen images of known products included in the training set. \textit{Figure \ref{figure: v2knownproducts}} shows how the IoU behaves over all of the images. Overall, the average IoU is good for all of these 4 products but the Alberto Balsam has 4 outliers points which are under 0.85.% The reason for that is 6 automatic labelled images didn't have good annotation and they were images 34, 49, 70, 72, 82, and 92 of the Alberto Balsam bottles.

Results in \textit{Table \ref{tab:v2ready}} show that the trained neural network has a 100\% True Positive rate, which means that the detection test always finds the right item. The average IoU for known products is 95.9\% which tells us that the neural network was trained right.

\textit{Figure \ref{fig:v2boxknownproducts}} show how the results from the detection run summarize in a box plot. Nivea Cleansing Milk has the highest IoU value, and Alberto Balsam has the lowest value. As mentioned before the Alberto Balsam has 4 outliers, which are shown on the box plot.

\textit{Figure \ref{figure: v2bestworst}} shows the highest and lowest IoU score on the single known items when using the second neural network. The lowest IoU is on the image that is right annotated, but the neural network finds a bad bounding box (red bounding box) and it does not match with the automatic annotation (green bounding box).


\subsubsection{On unknown Beiersdorf products}
In \textit{Section \ref{subsec:v2resunknownprod}} it can be seen how the second neural network performs on images containing multiple unknown items from the Beiersdorf dataset. \textit{Figure \ref{figure: v2unknownproducts}} show how the IoU behaves over the unknown products, the average IoU is not as good for the unknown products. The highest average IoU is on item nr. 11 and the reason for that is that the item is like the items that were trained. The lowest average IoU is on item nr. 6. The reason for this is that the item has a cylindrical shape, which differs from the items used to train the network. These results are like the results from Sverrir Bjarnason thesis \cite{bjarnason_detecting_2021} and there also item nr. 5 and item nr. 6 have bad IoU and F-score.
%Detecting previously unseen objects without human intervention using neural networks y
\textit{Table \ref{tab:test2unknown}} shows how the trained neural networks worked on the unknown products. It shows that item nr. 11 and nr. 12 has the best results. Item 11 has the best IoU but item 12 has a better F-score or 84.8\%. The model has the most trouble with the cylindrical-shaped items, nr. 5 and nr. 6. From those results, it is possible to compare these results to the results on the known items. The average IoU for known items was 95.9\% compared to an average IoU of 63.0\% when working on unknown items. That would possibly mean that the neural network would need better training images. 

In \textit{Figure \ref{fig:v2unknowniou}} is a box plot for each product that provides a visual summary of the results in \textit{Table \ref{tab:test2unknown}}. In the \textit{Figure \ref{fig:v2unknownioua}} it can be seen that the second neural network works least well on items 5 and 6. In the \textit{Figure \ref{fig:v2unknownioub}} it can be seen that the second neural network works least on item 5, 6, and 14, and it strange that item 14 or better known as Alberto Balsam gives a low F-score since that is the only bottle that was in the first dataset and was also trained with the neural network. The reason for these bad results on the Alberto Balsam bottle is that those images are taken from a greater distance from the bin, so there are some objects in the background that interrupt the network. 

\textit{Figure \ref{fig:v2bottles}} shows how the trained second neural network performed for a different number of items in the bin. In this box plot, the X-axis is the number of bottles in the bin, then it could be seen how the IoU changes while the number of bottles increases in the bin. It can be seen that the best IoU is when there is only one bottle in the bin and decreases when a bottle is added to the bin. This is to be expected as the second network model was only trained on images with one object in the bin and always had a gray background, so when that color changed the network got into a problem.

\textit{Figure \ref{fig:v2unknowniou}} and \textit{Figure \ref{fig:v2max}} shows visually how the second neural network performs on the Beiersdorf dataset. It shows the highest and lowest IoU and also the highest TP and highest FP. The highest IoU was on item nr.13 with an IoU score of 0.9862. The highest TP was on item nr.2 or 7 TP. Both items nr.13 and nr.2 were similar in size and shape to the items in the dataset on which the neural network was trained.

\subsubsection{On unknown Beiersdorf products - one detection per image}
This neural network was only trained on images with one annotation per image and considering that the data from the detection run was changed so it would be possible to see how it performs finding just one item per image. \textit{Table \ref{tab:v2zero}} shows the results from the detection run was, when assuming max one detection in each image. From \textit{Table \ref{tab:v2zero}} it can be seen that the IoU, precision, recall and F-score is higher than values from \textit{Table \ref{tab:test2unknown}} which had the raw data.


\subsection{Training the third neural network}
The third neural was trained from the second neural network on an automatically generated dataset, \textit{Figure \ref{fig:v3neuralnetwork}} shows how the IoU score developed over the number of epochs. In that figure, it is possible to see that the IoU rises quickly and then the IoU line flattens.

\textit{Figure \ref{figure: v3beforeaftertraining}} shows how the second neural network and the third neural network perform visually with multiple objects in the bin. The third neural network returns more bounding boxes, but the third neural returns better IoU and better bounding boxes.

\subsubsection{On multiple known items}
\textit{Figure \ref{figure: v3knownproducts}} and \textit{Table \ref{tab:v3known}} shows how the third neural network performs on the automatically generated dataset with multiple objects in the bin (\textit{Section: \ref{sec:multidataset}}). The results return not so good values on known items, but the zero detection are skewing the results.

\textit{Figure \ref{fig:v3boxknownproducts}} shows results in a box plot, it shows the Nivea Elastic returns the best and also most consistent in the IoU score. The third neural network is fragile on known items when there are multiple objects in the bin. The highest and lowest IoU score can be seen visually in \textit{Figure \ref{figure: v3bestworst}}, and the highest IoU was on Nivea Elastic or 98.7\%. 

\subsubsection{On unknown Beiersdorf products}
In \textit{Section \ref{subsec:v3resunknownprod}} it can be seen how the third neural network performs on images containing multiple unknown items from the Beiersdorf dataset. \textit{Figure \ref{figure: v3unknownproducts}} show how the IoU behaves over the unknown products, the average IoU is not as good for the unknown products. 
\textit{Table \ref{tab:test3unknown}} shows how the third neural networks worked on the unknown products. It shows that item nr. 11 has the best results. Item 11 has the best IoU but item 12 has a better F-score or 84.8\%. The model has the most trouble with the cylindrical-shaped items, nr. 5 and nr. 6. From those results, it is possible to compare these results to the results on the known items. The average IoU for known items was 69.5\% compared to an average IoU of 41.8\% when working on unknown items. Both results quite low, since the zero detection affects the IoU.



In \textit{Figure \ref{fig:v3unknowniou}} is a box plot for each product that provides a visual summary of the results in \textit{Table \ref{tab:test3unknown}}. In the \textit{Figure \ref{fig:v3unknowniou}} it can be seen that the third neural network has changeable results since the upper and lower quartiles are far from each other.



\textit{Figure \ref{fig:v3bottles}} shows how the trained third neural network performed for a different number of items in the bin. In this box plot, the X-axis is the number of bottles in the bin, then it could be seen how the IoU changes while the number of bottles increases in the bin. It can be seen that the best IoU is when there is only one bottle in the bin and decreases when a bottle is added to the bin. Results from this box plot show that the third neural network performs better on multiple items in the bin than the second neural network.

\textit{Figure \ref{fig:v3unknowniou}} and \textit{Figure \ref{fig:v3max}} shows visually how the third neural network performs on the Beiersdorf dataset. It shows the highest and lowest IoU and the highest TP and highest FP. The highest IoU was on item nr.13 with an IoU score of 0.9918, which is higher than the other neural networks. The highest TP was on item nr.12 or 4 TP. Both items nr.13 and nr.12 were similar in size and shape to the items in the dataset on which the neural network was trained.

\subsubsection{On unknown Beiersdorf products - one detection per image}
The first two datasets created by the robot are very different from the Beiersdorf dataset. The automatically created dataset shows for each image which item to pick while the Beiersdorf dataset shows all the items in the image which is a different task. The results of using the same CNN model for all datasets can be more easily compared if only one detection per image is used. This neural network was only trained on images with one annotation per image and considering that the data from the detection run was changed so it would be possible to see how it performs finding just one item per image. \textit{Table \ref{tab:v3zero}} shows the results from the detection run was, when assuming max one detection in each image. From \textit{Table \ref{tab:v3zero}} it can be seen that the IoU, precision, recall and F-score is higher than values from \textit{Table \ref{tab:test3unknown}} which had the raw data.

When comparing the results from the second neural network (\textit{Table \ref{tab:v2zero}}) and the third neural network (\textit{Table \ref{tab:v3zero}}), it can be seen that the IoU is higher for the third neural network or 76.4\% versus 63.9\% using the second neural network. But the second neural network has higher precision, recall, and F-score and that is because the second neural finds more detections or 2163 out of 2175 images versus the third neural network has a detection rate of 1166 out of 2175 images. 
\textit{Figure \ref{fig:v3zerofig}} is a box plot that shows the IoU for different products and the IoU for a different number of items in the bin. When comparing \textit{Figure \ref{fig:v3zeroc}} from the third neural network and the \textit{Figure \ref{fig:v2zeroc}} from the second neural network, it can be seen that the third neural network improved from the second neural network when there are multiple items in the bin. This indicates that the trained networks are getting better and can be improved by adding even more automatically generated images with multiple objects in the bin.

\subsection{Comparison of three trained neural networks}
\textit{Figure \ref{fig:comparison1}} shows a comparison of the average IoU and F-score was for the three trained neural networks on the Beiersdorf dataset \textit{(Sec: \ref{sec:beiersdorfdataset})}. The third neural network has the highest average IoU, which indicates that the third neural network returns the best results. But it has the lowest F-score which indicates it is not as good as the other neural networks finding the bottles but returns better IoU when it does find them.
%\textit{Figure \ref{fig:comparison2}} shows a comparison of the IoU for different number of items in the bin for the three trained neural networks on the Beiersdorf dataset \textit{(Sec: \ref{sec:beiersdorfdataset})}. The figure shows that the third neural network has the highest IoU every time. 



\clearpage
\chapter{Conclusion}\label{sec:conclusions}
%\fxfatal{Klára að skrifa - Conclusion}
%Tilgátan endurtekin. Niðurstöður tilrauna teknar saman og dregnar af þeim ályktanir. Hvað segja niðurstöðurnar um spurningarnar og tilgátuna, sem sett var fram í inngangi?
%Hvað segja þær ekki?
This project set out to look into the possibility of using a robot manipulator to generate training data for previously unseen objects. But to reach that conclusion, the objectives of the project had to be divided into concise sections, which included three research questions which the project aimed to answer. These research questions were as follows: i) Is it possible to generate new arrangements of objects using a robot manipulator? ii) Is it possible to annotate objects automatically, determining the extent of the objects in the images? iii) Is it possible to improve the performance of a Convolutional Neural Network using automatically generated training data from a robot?

%i. Is it possible to generate new arrangements of objects using a robot manipulator?
The results of this project showed that the robot is capable of moving and rotating objects around inside the bin to create new arrangements. It takes time for a human to move and pick objects, using a robot would save both a huge amount of time and manpower in making new arrangements.
%ii. Is it possible to annotate objects automatically, determining the extent of the objects in the images?
In machine learning annotation plays a huge part in object detection, but it is time-consuming for a human to annotate.
The automatically annotated images show a good annotation almost every time, which saves time and effort. 
Using automatic annotation would save time for those who need to improve their neural network but don't want to spend months or years on annotations.
%iii. Is it possible to improve the performance of a Convolutional Neural Network using automatically generated training data from a robot?
From the results on the trained neural networks in this project, it can be concluded that there is a possibility to train an excellent neural network by using only automatically generated training data. 
These trained neural networks can empty a bin with known items.
From those results, it can be concluded that the neural networks can be trained on automatically generated data and can be used to create even more automatically generated data. 
The results also show that the neural networks perform reasonably well on items that the neural network was not trained on, which indicates that the neural networks would be able to find some unknown items in a bin.

%% Future work As the field of object detection has endless possibilities and the work done here only gives an overview of potential solutions to a specific problem, there is much work that can be done by exploring one or more solutions presented in this project.
%nefna með annotation
This project shows us that there is a possibility to use a robot manipulator to create training data, but there are a few things that would be interesting to look further into. First, automatically annotating the objects was the hardest part of this project, and that would need some improvement with some new methods. Second, capture more images for each product and train a neural network with a larger dataset which would possibly make the neural networks better. Third, it would be a possibility to program the robot differently and use the depth camera to find outlines of objects.
